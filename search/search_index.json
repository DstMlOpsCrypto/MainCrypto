{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CryptoPredict","text":"<p>This project represent the final project of DataScientest MLOps exam. This project start in July 2024 to mid November 2024</p>"},{"location":"#rational","title":"Rational","text":"<p>The purpose of this project have 1 Objectives : Develop a fully functional Bitcoin price prediction application 'CryptoPredict'</p> ContraintsTeam membersProject management <p>Some constraints have been listed by DataSciencetest Team :</p> <ul> <li>Use only 1 VM on AWS EC2 (TODO: flavors)</li> <li>Required tools to use<ul> <li>MLFlow</li> <li>Airflow</li> <li>Docker and Docker Compose</li> <li>Prometheus and Kibana</li> <li>Web Framework (we took Streamlit)</li> <li>FastAPI</li> <li>Github with Github Actions</li> </ul> </li> <li>1 Deep Learning model with re trained mecanism</li> <li>logging mecanism</li> </ul> <ul> <li>Frederic</li> <li>Tristan</li> <li>Yann</li> </ul> <p>Github have been used for : </p> <ul> <li>Code repository</li> <li>Project management</li> <li>Testing and deployment through Github actions</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<p>The following prerequisites are minimal requirements to make this repository work:</p> <ul> <li>VM with proper network configuration to allow internal external flow</li> <li>Access to Github Actions</li> <li>Set github secrets for target repo<ul> <li>PRIVATE_KEY : SSH private key</li> <li>EC2_HOST: VM host</li> <li>EC2_USER: VM user</li> <li>GIT_PAT: user token</li> </ul> </li> <li>VS Code or any IDE</li> <li>python environnement setup</li> <li>git setup</li> </ul>"},{"location":"#launch-apps-locally","title":"Launch apps locally","text":"<ul> <li>clone repo</li> </ul> <pre><code>git clone https://github.com/DstMlOpsCrypto/MainCrypto.git\ncd MainCrypto.git\n</code></pre> <ul> <li>chmod +x 'setup.sh' file</li> </ul> <p>You are ready to go !</p> <p>The installation process is straightforward and simple, you just need to execute 'setup.sh'</p> <pre><code>sh setup.sh\n</code></pre> Tasks-workflow <pre><code>flowchart\n    subgraph install-docker-env;\n        local_docker_install.sh;\n    end;\n    subgraph refresh-containers;\n        clean-env --&gt; init-airflow;\n        init-airflow --&gt; launch-docker-app;\n    end;\n\n    install-docker-env --&gt; refresh-containers</code></pre> <p>1. Install-docker-env</p> <pre><code>- Prepare an environnement for linux (Debian, Fedora, CentOS, RedHat Entreprise, OpenSUSE, ArchLinux)\n- Add current user to the docker group\n</code></pre> <p>2. Refresh-containerized-apps-installation</p> <pre><code>- stop all containers\n- remove all stopped containers\n- remove all images\n- remove any volumes\n- remove any networks\n- remove all unused data\n- remove ./plugins folder (mapped)\n- remove ./dags folder (mapped)\n</code></pre> <p>3. Install apps from docker-compose.yml</p> <pre><code>- init airflow\n- launch containers\n</code></pre>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>    README.md               # TODO: details instalation instruction \n    API/                    # TODO: Main API service\n    PredictionAPI/          # Prediction API (see API)\n    airflow/                # TODO: Data flow orchestration\n        dags/\n    db/                     # TODO: Database (crypto data, assets, users)\n    frontend/               # TODO: Frontend (Streamlit)\n        components/\n        pages/\n        utils/\n    mlflow/                 # TODO: Models Lifecycle management (See modeling)\n    notebooks/              # TODO: Models scripts (See modeling)\n        exploration/\n        modeling/\n    prom/                   # TODO: Monitoring Service including Prometheus annd Grafana\n        alertmanager/\n        statsd/\n    scripts/                # TODO: Models scripts for training, predicting and evaluating model \n    src/                    # TODO: Ressources and modules useful for model scripts \n        data/\n        deployment/\n        evaluation/\n        features/\n        models/\n        utils/\n        vizualisation/\n    tests/                  # TODO: Tests repository for dev, including api, crypto datbase and modelling\n        api/\n        crypto/\n        model/\n    setup.sh                # bash script to install complete application\n    local_docker_clean.sh   # Bash script to clean docker\n    docker-compose.yml      # Production compose (see XX)\n    docker-compose-dev.yml  # Development compose (see XX)\n</code></pre>"},{"location":"#down-the-rabbit-hole","title":"Down the rabbit hole","text":"<p>We encourage anyone to take time to refer to dedicated pages listed on header.</p>"},{"location":"airflow/","title":"Airflow","text":"<p>This part presents an overview of Airflow tool for educational purposes.</p> CreatorUse Cases <ul> <li>Name: Maxime Beauchemin</li> <li>Background: Ex-Airbnb and ex-Facebook engineer</li> <li>Contribution: Developed Apache Airflow at Airbnb to manage complex workflows</li> </ul> <ul> <li> <p>Data Engineering</p> <ul> <li>ETL/ELT Pipelines: Extract, Transform, Load (Extract, Load, Transform) data from various sources.</li> <li>Data Processing: Automate data cleaning, transformation, and loading processes.</li> </ul> </li> <li> <p>Machine Learning</p> <ul> <li>MLOps: Orchestrate the entire machine learning lifecycle, including data preparation, model training, and deployment2.</li> <li>Model Training: Schedule and monitor machine learning model training jobs.</li> </ul> </li> <li> <p>Operational Analytics</p> <ul> <li>Automated Reporting: Generate and distribute reports on a scheduled basis.</li> <li>Real-time Analytics: Process and analyze data in real-time for timely insights.</li> </ul> </li> <li> <p>DevOps</p> <ul> <li>Infrastructure Management: Manage infrastructure provisioning and de-provisioning.</li> <li>Backup and Restore: Schedule regular backups and automate restore processes.</li> </ul> </li> <li> <p>**Custom Workflows  </p> <ul> <li>Ad Hoc Tasks: Run one-off tasks that are not tied to a schedule.</li> <li>Event-Driven Workflows: Trigger tasks based on external events, such as file uploads or API calls5.</li> </ul> </li> </ul>"},{"location":"airflow/#introduction-to-apache-airflow","title":"Introduction to Apache Airflow","text":"<p>Apache Airflow is\u2014a platform to programmatically author, schedule, and monitor workflows.</p> <p>Use Cases: Highlight common use cases, such as ETL processes, data pipeline automation, and machine learning workflows.</p>"},{"location":"airflow/#key-points","title":"Key Points","text":"<ul> <li>Open-source workflow management tool</li> <li>Focus on scalability and flexibility</li> <li>Suitable for batch-oriented workloads</li> </ul>"},{"location":"airflow/#main-characteristics-of-apache-airflow","title":"Main Characteristics of Apache Airflow","text":""},{"location":"airflow/#directed-acyclic-graphs-dags","title":"Directed Acyclic Graphs (DAGs)","text":"<p>workflows are defined as DAGs, ensuring tasks are executed in a specific order without cycles.</p>"},{"location":"airflow/#task-instances","title":"Task Instances","text":"<p>Discuss how each task within a DAG is an instance, allowing for granular monitoring and control.</p>"},{"location":"airflow/#scheduler","title":"Scheduler","text":"<p>The Airflow scheduler is essential for managing the execution of tasks in a workflow, ensuring that tasks run at the specified intervals and in the correct order1. Its ability to handle dependencies, manage concurrency, and handle failures makes it a powerful tool for orchestrating complex data pipelines.</p>"},{"location":"airflow/#monitoring-dags-and-tasks","title":"Monitoring DAGs and Tasks","text":"<p>The scheduler continuously monitors all Directed Acyclic Graphs (DAGs) and their tasks. It keeps track of the state of each task and DAG, ensuring that tasks are executed in the correct order based on their dependencies.</p>"},{"location":"airflow/#scheduling-tasks","title":"Scheduling Tasks","text":"<p>The scheduler uses the schedule_interval parameter defined in each DAG to determine when to run tasks. This parameter can be a cron expression or a timedelta object. </p> <p>For example, a DAG with a schedule_interval of '@daily' will run once a day at midnight.</p>"},{"location":"airflow/#triggering-task-runs","title":"Triggering Task Runs","text":"<p>When the scheduler determines that a task should be run (based on the schedule_interval and task dependencies), it triggers the task instance. This involves creating a DAG run and scheduling the tasks within that run.</p>"},{"location":"airflow/#triggering-task-runs_1","title":"Triggering Task Runs","text":"<p>When the scheduler determines that a task should be run (based on the schedule_interval and task dependencies), it triggers the task instance. This involves creating a DAG run and scheduling the tasks within that run.</p>"},{"location":"airflow/#handling-dependencies","title":"Handling Dependencies","text":"<p>The scheduler ensures that tasks are executed only when their dependencies have been met. This means that if Task B depends on Task A, Task B will not start until Task A has successfully completed.</p>"},{"location":"airflow/#managing-concurrency","title":"Managing Concurrency","text":"<p>The scheduler enforces concurrency limits to prevent overloading the system. It checks the number of currently running tasks and ensures that the number of new tasks scheduled does not exceed the configured limits.</p>"},{"location":"airflow/#handling-failures-and-retries","title":"Handling Failures and Retries","text":"<p>If a task fails, the scheduler can automatically retry the task based on the retry configuration specified in the DAG. It manages retries and ensures that failed tasks are retried according to the defined retry policy.</p>"},{"location":"airflow/#continuous-operation","title":"Continuous Operation","text":"<p>The scheduler runs as a persistent service in a production environment. It continuously checks for new DAG runs and schedules tasks as needed. To start the scheduler, you simply run the command airflow scheduler.</p>"},{"location":"airflow/#backfilling-and-catchup","title":"Backfilling and Catchup","text":"<p>Airflow can backfill data by running tasks for past intervals that have not been executed. This is useful for populating historical data. The scheduler can also catch up on missed runs if the system was down or tasks were delayed.</p>"},{"location":"airflow/#executor-types","title":"Executor Types","text":"<p>In Apache Airflow, the Executor is responsible for managing how and where tasks are run. Different executors are designed to cater to different scalability and deployment needs. </p> <p>Here are the main types of executors in Airflow:</p>"},{"location":"airflow/#sequentialexecutor","title":"SequentialExecutor","text":"<p>The simplest executor, which runs tasks sequentially, one at a time. Use Case: Suitable for development and testing environments where only one task needs to be run at a time.</p>"},{"location":"airflow/#pros","title":"Pros:","text":"<ul> <li>Easy to set up and use.</li> <li>Minimal resource requirements.</li> </ul>"},{"location":"airflow/#cons","title":"Cons:","text":"<p>Not suitable for production or environments that require parallel task execution.</p>"},{"location":"airflow/#localexecutor","title":"LocalExecutor","text":"<p>Executes tasks in parallel using multiple worker processes on a single machine. Use Case: Suitable for small to medium-scale deployments where parallelism is required but resources are limited to a single machine.</p>"},{"location":"airflow/#pros_1","title":"Pros:","text":"<ul> <li>Easy to set up.</li> <li>Supports parallel execution.</li> </ul>"},{"location":"airflow/#cons_1","title":"Cons:","text":"<ul> <li>Limited by the resources of a single machine.</li> <li>May not scale well for very large workloads.</li> </ul>"},{"location":"airflow/#celeryexecutor","title":"CeleryExecutor","text":"<p>Uses Celery to distribute task execution across multiple worker nodes. This executor is suitable for larger-scale, distributed environments. Use Case: Ideal for production environments that need to scale out task execution across multiple machines.</p>"},{"location":"airflow/#pros_2","title":"Pros:","text":"<ul> <li>Highly scalable and flexible.</li> <li>Supports distributed execution and fault tolerance.</li> </ul>"},{"location":"airflow/#cons_2","title":"Cons:","text":"<ul> <li>Requires additional components like a message broker (RabbitMQ, Redis) and a backend database (e.g., PostgreSQL, MySQL).</li> <li>More complex to set up and maintain compared to simpler executors.</li> </ul>"},{"location":"airflow/#kubernetesexecutor","title":"KubernetesExecutor","text":"<p>Runs each task in a separate Kubernetes pod, leveraging Kubernetes for scalability and orchestration. Use Case: Best suited for environments that already use Kubernetes for container orchestration and need to scale Airflow tasks dynamically.</p>"},{"location":"airflow/#pros_3","title":"Pros:","text":"<ul> <li>Dynamic scaling based on Kubernetes.</li> <li>Isolation of tasks in separate pods enhances security and resource management.</li> </ul>"},{"location":"airflow/#cons_3","title":"Cons:","text":"<ul> <li>Requires a Kubernetes cluster.</li> <li>More complex to configure and manage.</li> </ul>"},{"location":"airflow/#daskexecutor","title":"DaskExecutor","text":"<p>Dask is a parallel computing library to execute tasks. DaskExecutor is designed for distributed computing environments. Use Case: Suitable for environments that use Dask for parallel processing and need to integrate Airflow with Dask\u2019s distributed computing capabilities.</p>"},{"location":"airflow/#pros_4","title":"Pros:","text":"<ul> <li>Leverages Dask's distributed computing power.</li> <li>Can scale out tasks across multiple workers.</li> </ul>"},{"location":"airflow/#cons_4","title":"Cons:","text":"<ul> <li>Requires knowledge of Dask.</li> <li>May be more complex to set up compared to LocalExecutor or SequentialExecutor.</li> </ul>"},{"location":"airflow/#key-point","title":"Key Point","text":"<p>Each executor in Airflow is designed to address different needs and scale levels. Here\u2019s a quick comparison to help you choose the right executor for your use case:</p> <ul> <li>SequentialExecutor: Best for development and testing.</li> <li>LocalExecutor: Good for small to medium-scale deployments on a single machine.</li> <li>CeleryExecutor: Ideal for large-scale, distributed production environments.</li> <li>KubernetesExecutor: Perfect for Kubernetes-based environments needing dynamic scaling.</li> <li>DaskExecutor: Suitable for integrating with Dask for distributed computing.</li> </ul> <p>Choosing the right executor depends on your specific requirements, available resources, and scalability needs.</p>"},{"location":"airflow/#architecture-of-apache-airflow","title":"Architecture of Apache Airflow","text":""},{"location":"airflow/#components-overview","title":"Components Overview","text":"<ul> <li>Scheduler: Manages task execution</li> <li>Executor: Executes tasks</li> <li>Worker: Executes the tasks, can be scaled horizontally</li> <li>Metadata Database: Stores state information of DAGs and tasks</li> <li>Web Server: Provides a UI for monitoring and managing workflows</li> </ul>"},{"location":"airflow/#key-points_1","title":"Key Points","text":"<ul> <li>Centralized architecture for monitoring and control</li> <li>Scalable components to handle large workloads</li> <li>Integration with various backends and services</li> </ul>"},{"location":"airflow/#detailed-architectural-breakdown","title":"Detailed Architectural Breakdown","text":"<p> source : Bageshwar Kumar</p>"},{"location":"airflow/#scheduler_1","title":"Scheduler:","text":"<p>The scheduler is the heart of Airflow, responsible for scheduling the execution of tasks. Here\u2019s how it works:</p> <ul> <li> <p>Monitoring: Continuously monitors DAGs (Directed Acyclic Graphs) and their associated tasks.</p> </li> <li> <p>Task Triggering: Determines when tasks should be run based on their schedule_interval and dependencies.</p> </li> <li> <p>Concurrency Management: Manages the execution concurrency to ensure system stability.</p> </li> <li> <p>Fault Tolerance: Handles task retries and failures, ensuring tasks are re-executed as configured.</p> </li> </ul> <p>The scheduler is crucial for ensuring that tasks are executed at the right time and in the correct order.</p>"},{"location":"airflow/#executor","title":"Executor","text":"<p>The executor is responsible for executing the tasks scheduled by the scheduler. Airflow supports several types of executors, each designed for different use cases (see Executor).</p>"},{"location":"airflow/#workers","title":"Workers","text":"<p>Workers are the entities that execute the tasks assigned by the executor. Depending on the executor type:</p> <ul> <li> <p>LocalExecutor Workers: Use local processes to run tasks.</p> </li> <li> <p>CeleryExecutor Workers: Distributed across multiple nodes, each running Celery workers.</p> </li> <li> <p>KubernetesExecutor Workers: Use Kubernetes pods to run tasks.</p> </li> </ul> <p>Workers handle the actual execution of tasks, ensuring that they are completed successfully.</p>"},{"location":"airflow/#metadata-database","title":"Metadata Database","text":"<p>The metadata database stores all the state and configuration information for Airflow, including:</p> <ul> <li> <p>DAG Definitions: The structure and schedule of DAGs.</p> </li> <li> <p>Task Instances: The state of each task instance (e.g., success, failure, running).</p> </li> <li> <p>Logs and Metrics: Detailed logs of task executions and system metrics.</p> </li> <li> <p>User Information: User roles and access permissions.</p> </li> </ul> <p>Common databases used include PostgreSQL and MySQL. The metadata database is crucial for maintaining the state and history of workflows.</p>"},{"location":"airflow/#web-server","title":"Web Server","text":"<p>The web server provides a user interface for monitoring and managing Airflow workflows. Key features include:</p> <ul> <li> <p>DAG Visualization: Visual representation of DAGs and their task dependencies.</p> </li> <li> <p>Task Monitoring: Real-time status of task instances, including logs and execution details.</p> </li> <li> <p>Triggering and Pausing DAGs: Ability to manually trigger or pause DAGs.</p> </li> <li> <p>Access Control: User authentication and role-based access control.</p> </li> </ul> <p>The web server is built using Flask and offers an intuitive interface for interacting with Airflow.</p>"},{"location":"airflow/#key-points_2","title":"Key Points","text":"<ul> <li>DAG: User defines a DAG with its tasks and schedules it using a Python script.</li> <li>Scheduler: Heart of the Airflow, triggering task execution</li> <li>Executor: Executes tasks using different backend options</li> <li>Workers: Scalable nodes that carry out tasks</li> <li>Metadata Database: Maintains state and scheduling info</li> <li>Web Server: UI for workflow management</li> </ul>"},{"location":"airflow/#integration-and-extensions","title":"Integration and Extensions","text":"<p>Apache Airflow is highly extensible and integrates seamlessly with various tools and libraries, making it a powerful platform for orchestrating complex workflows.</p>"},{"location":"airflow/#plugins","title":"Plugins","text":"<p>Plugins in Airflow allow you to extend its core functionality by adding custom hooks, operators, sensors, executors, web views, and more. Plugins enable you to tailor Airflow to meet specific requirements of your workflows.</p>"},{"location":"airflow/#creating-a-plugin","title":"Creating a Plugin","text":"<p>Creating a plugin is achieve by defining a Python class that inherits from AirflowPlugin and adding custom components to it.</p> <pre><code>from airflow.plugins_manager import AirflowPlugin\nfrom airflow.operators.dummy_operator import DummyOperator\n\nclass MyCustomPlugin(AirflowPlugin):\n    name = \"my_custom_plugin\"\n    operators = [DummyOperator]\n</code></pre>"},{"location":"airflow/#custom-hooks-and-operators","title":"Custom Hooks and Operators","text":"<p>Custom hooks can interact with external systems and custom operators to define specific tasks.</p> <pre><code>from airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\nclass MyCustomOperator(BaseOperator):\n    @apply_defaults\n    def __init__(self, my_param, *args, **kwargs):\n        super(MyCustomOperator, self).__init__(*args, **kwargs)\n        self.my_param = my_param\n\n    def execute(self, context):\n        self.log.info(f\"Running with parameter: {self.my_param}\")\n</code></pre>"},{"location":"airflow/#operators","title":"Operators:","text":"<p>Operators are building blocks of workflows in Airflow. Each operator performs a specific task, such as executing a Python function, running a Bash script, or transferring data between systems.</p> <ul> <li> <p>Types of Operators: Airflow comes with a variety of built-in operators, such as:</p> </li> <li> <p>BashOperator: Runs a bash command.</p> </li> <li> <p>PythonOperator: Executes a Python callable.</p> </li> <li> <p>EmailOperator: Sends an email.</p> </li> <li> <p>SqlOperator: Executes an SQL command.</p> </li> </ul> <pre><code>from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\ndef my_function():\n    print(\"Hello from PythonOperator\")\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1)\n}\n\nwith DAG('my_dag', default_args=default_args, schedule_interval='@daily') as dag:\n    task = PythonOperator(\n        task_id='my_python_task',\n        python_callable=my_function\n    )\n</code></pre> <p>You can extend the BaseOperator class to create custom operators that cater to specific needs.</p>"},{"location":"airflow/#sensors","title":"Sensors","text":"<p>Sensors are a special type of operator that waits for a certain condition to be met before proceeding. They are useful for tasks that depend on external events or conditions.</p> <ul> <li> <p>Built-in Sensors: Airflow includes several built-in sensors, such as:</p> </li> <li> <p>ExternalTaskSensor: Waits for a task in another DAG to complete.</p> </li> <li> <p>FileSensor: Waits for a file to appear in a specified location.</p> </li> <li> <p>HttpSensor: Waits for an HTTP endpoint to return a certain status.</p> </li> </ul> <pre><code>from airflow import DAG\nfrom airflow.operators.sensors import FileSensor\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1)\n}\n\nwith DAG('my_sensor_dag', default_args=default_args, schedule_interval='@daily') as dag:\n    wait_for_file = FileSensor(\n        task_id='wait_for_file',\n        filepath='/path/to/file',\n        poke_interval=30,  # Check every 30 seconds\n    )\n</code></pre> <p>custom sensors by extending the BaseSensorOperator class and implementing the poke method. The poke method is called repeatedly by the sensor until it returns True. If it returns False, the sensor will wait for a specified interval (defined by poke_interval) before checking again. This process continues until the sensor times out (defined by timeout) or the condition is met.</p>"},{"location":"airflow/#benefits-and-limitations","title":"Benefits and Limitations","text":""},{"location":"airflow/#benefits","title":"Benefits","text":"<ul> <li>Highly extensible and customizable</li> <li>Excellent for orchestrating complex workflows</li> <li>Scalable to handle increasing workloads</li> <li>Strong community support</li> </ul>"},{"location":"airflow/#limitations","title":"Limitations","text":"<ul> <li>Can be complex to set up and manage</li> <li>Requires knowledge of Python for creating DAGs</li> <li>Potential performance issues with very large DAGs</li> </ul>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#capability","title":"Capability","text":"<p>This diagram provides a structured view of the capabilities and sub-capabilities involved in a cryptocurrency prediction application, or at least how we imagine it !</p> Info <p>All sub capabilities surrounded by red line are note implemented in ths Proof Of Concept (POC).</p> <p></p>"},{"location":"architecture/#data-collection","title":"Data Collection","text":"<p>This capability involves gathering data from various sources, which is essential for making accurate predictions.</p>"},{"location":"architecture/#sub-capabilities","title":"Sub-capabilities","text":"<ul> <li> <p>Real-time Market Data Retrieval: Continuously collecting current price, volume, and other relevant market data from exchanges and financial platforms.</p> </li> <li> <p>Historical Data Access: Accessing past market data to identify patterns and trends over time.</p> </li> <li> <p>API Integration for Data Sources: Connecting with external APIs to fetch data from different exchanges, news platforms, social media, and more.</p> </li> </ul>"},{"location":"architecture/#data-processing","title":"Data Processing","text":"<p>Once data is collected, it needs to be processed to make it suitable for analysis and model training.</p>"},{"location":"architecture/#sub-capabilities_1","title":"Sub-capabilities","text":"<ul> <li> <p>Data Cleaning and Normalization: Removing noise and inconsistencies in the data, and normalizing it to a standard format.</p> </li> <li> <p>Feature Extraction and Selection: Identifying and selecting the most relevant features that will be used in model training.</p> </li> <li> <p>Data Transformation and Aggregation: Transforming data into a structured format and aggregating it to make it suitable for analysis.</p> </li> </ul>"},{"location":"architecture/#model-training","title":"Model Training","text":"<p>Training predictive models using the processed data to forecast future cryptocurrency prices and trends.</p>"},{"location":"architecture/#sub-capabilities_2","title":"Sub-capabilities","text":"<ul> <li> <p>Selection of Machine Learning Algorithms: Choosing the appropriate machine learning algorithms for prediction (e.g., LSTM, Random Forest, SVM).</p> </li> <li> <p>Training and Validation of Models: Training the models using historical data and validating their performance using a validation set.</p> </li> <li> <p>_Hyperparameter Tuning: Adjusting the model parameters to optimize performance.</p> </li> </ul>"},{"location":"architecture/#prediction-generation","title":"Prediction Generation","text":"<p>Generating predictions based on the trained models.</p>"},{"location":"architecture/#sub-capabilities_3","title":"Sub-capabilities:","text":"<ul> <li> <p>Real-time Prediction Generation: Producing predictions in real-time based on current market data.</p> </li> <li> <p>Batch Prediction Processing: Running predictions in batches for historical analysis or reporting.</p> </li> <li> <p>Integration with Prediction APIs: Exposing prediction functionalities through APIs for other applications to consume.</p> </li> </ul>"},{"location":"architecture/#user-interface","title":"User Interface","text":"<p>Providing an interface for users to interact with the application, view predictions, and customize their experience.</p>"},{"location":"architecture/#sub-capabilities_4","title":"Sub-capabilities","text":"<ul> <li> <p>Dashboard for Visualizing Predictions: Creating interactive dashboards to display predictions, trends, and insights.</p> </li> <li> <p>Alerts and Notifications: Sending real-time alerts and notifications for significant market movements.</p> </li> <li> <p>User Customization Options: Allowing users to customize their dashboards and set preferences for alerts and notifications.</p> </li> </ul>"},{"location":"architecture/#security","title":"Security","text":"<p>Ensuring that the application and data are secure from unauthorized access and breaches.</p>"},{"location":"architecture/#sub-capabilities_5","title":"Sub-capabilities","text":"<ul> <li> <p>Data Encryption and Protection: Encrypting data at rest and in transit to protect it from unauthorized access.</p> </li> <li> <p>Secure API Access: Implementing security measures like API keys and authentication to secure API endpoints.</p> </li> <li> <p>Anomaly Detection and Response: Detecting and responding to anomalies in data access and usage.</p> </li> </ul>"},{"location":"architecture/#scalability","title":"Scalability","text":"<p>Ensuring that the application can handle increased loads and grow as needed.</p>"},{"location":"architecture/#sub-capabilities_6","title":"Sub-capabilities","text":"<ul> <li> <p>Load Balancing and Resource Management: Distributing workload across multiple servers to ensure optimal performance.</p> </li> <li> <p>Horizontal Scaling of Services: Adding more instances of services to handle increased demand.</p> </li> <li> <p>Efficient Data Storage Solutions: Implementing scalable and efficient storage solutions to manage large volumes of data.</p> </li> </ul>"},{"location":"architecture/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":"<p>Regularly monitoring and maintaining the application to ensure it runs smoothly.</p>"},{"location":"architecture/#sub-capabilities_7","title":"Sub-capabilities","text":"<ul> <li> <p>Performance Monitoring: Continuously monitoring the performance of the application and its components.</p> </li> <li> <p>Log Management and Analysis: Collecting and analyzing logs to identify and troubleshoot issues.</p> </li> <li> <p>Regular Model Retraining and Updates: Regularly updating the models with new data to ensure their accuracy and relevance.</p> </li> </ul> <p>Our standard methodology has involved several key steps:  - data collection - data loading and preprocessing (normalization, transformation) - model creation - model training - model prediction and evaluation - model deployment. </p>"},{"location":"architecture/#architecture-layers","title":"Architecture layers","text":""},{"location":"architecture/#business-user-process","title":"Business (user process)","text":"<p>The user journey refers to the sequence of steps a user takes to interact with our application. In the context of a cryptocurrency prediction application with a FastAPI front end, the user journey can be broken down into several stages. </p> <p></p>"},{"location":"architecture/#applicative","title":"Applicative","text":"<p>This diagram illustrate all apps used in our application.</p> <p></p>"},{"location":"architecture/#technology","title":"Technology","text":"<p>When choosing infrastructure containers for a cryptocurrency prediction project, several factors come into play to ensure scalability, reliability, and efficiency. </p> <p>This project can be view a a POC and we did not use Orchestration with Kubernetes, we used Docker and Docker-Compose on a single VM.</p> <p>This schema illustrate the containerized structure of the application.</p> <p></p>"},{"location":"architecture/#streamlit","title":"Streamlit","text":"<p>Streamlit is our frontend application. It's the only service available only on the public network. It shares the public network with the gateway API which is used to access the private services securely. It's exposed on the port 8501.</p>"},{"location":"architecture/#structure","title":"Structure","text":"<pre><code>RepoCrypto/frontend/\n\u251c\u2500\u2500 Dockerfile              # Streamlit container configuration\n\u251c\u2500\u2500 app.py                  # Streamlit entry point\n\u251c\u2500\u2500 requirements.txt        # Python dependencies\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 api_client.py      # API client functions\n\u2502   \u2514\u2500\u2500 auth.py            # authentification functions\n\u2514\u2500\u2500 pages/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 account.py         # manage his account\n    \u251c\u2500\u2500 administration.py  # manage user accounts\n    \u251c\u2500\u2500 create_user.py     # create a new user\n    \u251c\u2500\u2500 data_analysis.py   # data analysis of historical data\n    \u251c\u2500\u2500 home.py            # home page\n    \u251c\u2500\u2500 model.py           # model management\n    \u2514\u2500\u2500 predictions.py     # predictions visualization\n</code></pre> <p>Some pages are only accessible for admin users, such as administration.py and create_user. Other pages are accessible for all users. </p> <p>The frontend use the role and the token to manage the access to the pages.</p> <p>The only service with which the frontend communicate is the gateway API. It's throught the API that the frontend can access authentification / authorization services and backend features.</p> <p>In streamlit we used plotly to create the charts and display the data.</p> <p>Below is the scheme of Streamlit features through the API Gateway endpoints</p> <p></p>"},{"location":"architecture/#api-management","title":"API management","text":"<p>in our project, we decided to have 2 FastAPI applications, one is only on a private network for security reasons and the other one is on both private and public networks to work as a bridge between the frontend (streamlit), the private API and other services.</p> <p>Please see below an illustration of our API architecture</p> <p></p> <ul> <li>Private API : The goal of our private API is to communicates with airflow to trigger training and prediction tasks.</li> </ul>"},{"location":"architecture/#prediction-api","title":"Prediction API","text":"<pre><code>PredictionAPI/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py          # FastAPI application entry point\n\u2502   \u251c\u2500\u2500 registry.py      # Prometheus metrics configuration\n\u2502   \u2514\u2500\u2500 prediction/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 router.py    # Prediction endpoints and logic\n\u251c\u2500\u2500 Dockerfile          # Container configuration\n\u251c\u2500\u2500 gunicorn_conf.py   # Gunicorn server settings\n\u251c\u2500\u2500 start-reload.sh \n\u251c\u2500\u2500 start.sh \n\u2514\u2500\u2500 requirements.txt   # Python dependencies\n</code></pre> <p>The Dockerfile configures the containerized environment for the Prediction API.  * It is used by the docker-compose file to start this service. </p> <ul> <li>It exposes port 3001, uses gunicorn_conf.py to configure the gunicorn server and requirements.txt to install the dependencies. </li> </ul> <p>The dockerfile is configured to use the start-reload.sh script to start the service in development mode (enabling hot reloading) and the start.sh script to start the service in production mode.</p> <p>Gunicorn_conf.py configures the Gunicorn WSGI server that runs the FastAPI application. It manages the number of workers, timeout, and other settings.</p> <p>main.py is the entry point of the FastAPI application. It initializes the application and sets up the necessary configurations. It sets up CORS middleware, implements rate limiting, configures Prometheus metrics middleware and registers routers.</p> <p>As we have FastAPI running with multiple Gunicorn workers, the request is load balanced across the workers. It also meands that registry is crucial for prometheus metrics to be scraped properly. </p> <p>Registry.py is used to configure the prometheus metrics which are  used in the main.py and router.py files. Router.py defines the routes and the logic behind them.</p>"},{"location":"architecture/#endpoints","title":"Endpoints","text":"<p>How requests are handled: <pre><code>graph LR\n    A[Client Request] --&gt; B[Docker Container]\n    B --&gt; C[Gunicorn]\n    C --&gt; D[FastAPI App]\n    D --&gt; E[Route Handlers]\n</code></pre></p> <p>we have several endpoints:</p> <ul> <li> <p>GET /metrics</p> <ul> <li>Response: Prometheus metrics in text format</li> <li>This endpoint provides monitoring metrics including:<ul> <li>prediction_api_request_count: Total requests</li> <li>prediction_api_request_latency_seconds: Request timing</li> <li>prediction_api_exception_count: Error tracking</li> <li>prediction_api_prediction_count: Prediction usage</li> <li>prediction_api_model_score: Model performance</li> </ul> </li> </ul> </li> <li> <p>GET /predict/latest-prediction</p> <ul> <li>Response: JSON object containing the latest prediction</li> <li>It uses the prediction saved in the database to avoid calling the model unnecessarily.</li> </ul> </li> <li> <p>GET /predict/model-evaluation</p> <ul> <li>Response: JSON object containing the model evaluation (MSE (train/test) and R\u00b2 score (train/test))</li> <li>It uses the evaluation saved in the database to avoid calling the model unnecessarily.</li> </ul> </li> <li> <p>GET /predict/models</p> <ul> <li>Response: JSON object containing the list of available models</li> <li>It uses MLflow client to get the list of models.</li> </ul> </li> <li> <p>GET /predict/best-model</p> <ul> <li>Response: JSON object containing the best model</li> <li>It uses the best models in the database to return the best model based on the MSE.</li> </ul> </li> <li> <p>POST /predict/train</p> </li> <li>POST /predict/score</li> <li>POST /predict/predict<ul> <li>These endpoints are used to trigger the training, scoring and prediction tasks in airflow.</li> </ul> </li> </ul> <p>The endpoints works with: * PostgreSQL database * MLflow * Airflow * Prometheus</p>"},{"location":"architecture/#gateway-api","title":"Gateway API","text":"<p>The goal of our gateway API is to work as a bridge between the frontend and the private API and other services for security reasons.</p>"},{"location":"architecture/#structure_1","title":"Structure","text":"<pre><code>PredictionAPI/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py          # FastAPI application entry point\n\u2502   \u251c\u2500\u2500 database.py      # Database configuration\n\u2502   \u2514\u2500\u2500 authentication/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 security.py  # Token generation and verification functions\n\u2502       \u251c\u2500\u2500 utils.py     # Password hashing and verification functions\n\u2502       \u2514\u2500\u2500 router.py    # Authentication endpoints and logic\n\u2502   \u2514\u2500\u2500 crypto/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500  router.py    # Crypto endpoints and logic\n\u2502   \u2514\u2500\u2500 prediction/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 router.py    # Prediction endpoints and logic\n\u251c\u2500\u2500 Dockerfile          # Container configuration\n\u251c\u2500\u2500 gunicorn_conf.py   # Gunicorn server settings\n\u251c\u2500\u2500 start-reload.sh \n\u251c\u2500\u2500 start.sh \n\u2514\u2500\u2500 requirements.txt   # Python dependencies\n</code></pre> <p>The architecture is quite similar to the private API but it has some differences. It includes authentication and authorization mechanisms based on user roles, passwords and tokens. These enables us to protect sensitive and critical points.</p> <p>The crypto folder includes the endpoints to get the list of available cryptocurrencies, add new cryptocurrencies based on what's available in Kraken (our dataprovider) delete cryptocurrencies, get current prices or historical data. The prediction folder just had endpoints querying the private API to get the predictions, models, trigger training, scoring and prediction tasks...</p>"},{"location":"architecture/#endpoints_1","title":"Endpoints","text":"<p>The list of all endpoints:</p> <ul> <li>POST /auth/signup : Record new user</li> <li>POST /auth/login (not protected) : User loggin</li> <li>GET /auth/users/me : Modify user information profil by user</li> <li>PUT /auth/user/me : Check user account for user</li> <li>DELETE /auth/users/{username} : Delete users by admin</li> <li>PUT /auth/users/{username}/role : Modify user role by admin</li> <li>GET /auth/users : Get users list by admin</li> <li>GET /crypto/assets : Get all assets in db</li> <li>POST /crypto/assets : Add new asset</li> <li>GET /crypto/asset_history/{asset} : Get asset OHLC value history to date </li> <li>DELETE /crypto/assets/{asset_id} : Get asset OHLC value history to date </li> <li>GET /crypto/kraken_assets : Get list of assets from provider</li> <li>GET /crypto/asset_latest/{asset} : Get last OHCL value from Db</li> <li>GET /prediction/latest-prediction : Get last prediction for asset (BTC)</li> <li>GET /prediction/model-evaluation : Get last model evalution for asset (BTC)</li> <li>GET /prediction/best-model : Get prediction of best model for asset (BTC)</li> <li>GET /prediction/models : List all model experiments</li> <li>POST /prediction/train : Trigger Airflow DAG training</li> <li>POST /prediction/score : Trigger Airflow DAG scoring</li> <li>POST /prediction/predict : Trigger Airflow DAG prediction</li> </ul> <p>The endpoints works with: * Private API * PostgreSQL database * Airflow</p>"},{"location":"code-install/","title":"Code install","text":"<p>An exemple of codeblock </p> install...<pre><code> #!/bin/bash\n\n### clean docker\nbash local_docker_clean.sh\n\n### Install environnement\ndocker-compose --verbose up airflow-init\n\n### Launch docker compose\ndocker-compose --verbose up -d\n\n#wait 65s for containers to get ready\nsleep 65\ndocker container ls\n</code></pre>"},{"location":"content-tab/","title":"Content tab","text":""},{"location":"content-tab/#content-tab","title":"content tab","text":""},{"location":"content-tab/#generic","title":"generic","text":"plain plan2order <p>blablz</p> <ul> <li>first item</li> <li>second item</li> </ul> <ol> <li>item</li> <li>item</li> </ol> <p>ttt</p> <p>aaaaaaa</p> ttt <p>aaaaaaa</p> <pre><code>graph LR\n\nA[start] --&gt; B{failure?};\nB --&gt;|Yes| C[investigate...];\nC --&gt; D[debug];\nD --&gt; B;\nB ----&gt; |No| E[success];\n\n</code></pre> <p>https://www.youtube.com/watch?v=xlABhbnNrfI https://squidfunk.github.io/mkdocs-material/setup/extensions/python-markdown-extensions/</p>"},{"location":"fastapi/","title":"FastAPI","text":"<p>This part presents an overview of FastAPI tool for educational purposes.</p>"},{"location":"fastapi/#introduction-to-fastapi","title":"Introduction to FastAPI","text":"<p>FastAPI is\u2014a modern, fast (high-performance) web framework for building APIs with Python 3.6+ based on standard Python type hints.</p> CreatorUse Cases <ul> <li>Name: Sebasti\u00e1n Ram\u00edrez, also known as @tiangolo on GitHub</li> <li>Background: Sebasti\u00e1n is a software developer from Colombia who currently resides in Berlin, Germany</li> <li>Contribution: He is the creator of several popular open-source projects, including FastAPI, Typer, SQLModel, and Asyncer</li> </ul> <ul> <li> <p>API Development</p> <ul> <li>Building RESTful APIs: FastAPI is designed to create robust and efficient RESTful APIs with minimal code.</li> <li>GraphQL APIs: FastAPI supports GraphQL, allowing for flexible and powerful API designs.</li> </ul> </li> <li> <p>Microservices Development</p> <ul> <li>Microservices Architecture: FastAPI is ideal for developing microservices due to its simplicity and performance.</li> <li>Service-to-Service Communication: Facilitates communication between different microservices in a distributed system.</li> </ul> </li> <li>Real-time Applications<ul> <li>WebSockets: Supports real-time communication using WebSockets for applications like chat apps and live updates.</li> <li>Server-Sent Events (SSE): Enables server-to-client communication for real-time data streaming.</li> </ul> </li> <li>**Machine Learning and Data Science<ul> <li>Model Serving: Expose machine learning models as APIs for predictions and inference.</li> <li>Data Pipelines: Automate and orchestrate data processing and model training workflows.</li> </ul> </li> <li>Backend Services<ul> <li>Authentication and Authorization: Implement secure authentication and authorization mechanisms.</li> <li>CRUD Operations: Handle Create, Read, Update, and Delete operations for various resources.</li> </ul> </li> <li>Automation and Orchestration<ul> <li>Task Scheduling: Schedule and manage background tasks and jobs.</li> <li>Workflow Automation: Orchestrate complex workflows involving multiple steps and services.</li> </ul> </li> <li>Integration with Other Tools<ul> <li>Database Integration: Seamlessly integrate with databases like PostgreSQL, MySQL, and MongoDB.</li> <li>Third-Party Services: Connect with external APIs and services for enhanced functionality.</li> </ul> </li> <li>Custom Applications<ul> <li>Custom Business Logic: Implement specific business logic tailored to unique requirements.</li> <li>Prototyping: Quickly prototype and iterate on new ideas and features.</li> </ul> </li> </ul>"},{"location":"fastapi/#key-points","title":"Key Points","text":"<ul> <li>High performance, on par with Node.js and Go</li> <li>Easy to use and learn</li> <li>Based on standard Python type hints</li> </ul>"},{"location":"fastapi/#main-characteristics-of-fastapi","title":"Main Characteristics of FastAPI","text":""},{"location":"fastapi/#performance","title":"Performance:","text":"<ul> <li> <p>High Performance: Built on Starlette for the web parts and Pydantic for the data parts. By using asynchronous programming, FastAPI can handle more requests per second compared to many other frameworks. Benchmarks show that it is one of the fastest Python web frameworks available.</p> </li> <li> <p>Asynchronous Capabilities: FastAPI supports asynchronous request handling, enabling better performance, especially for I/O-bound operations.</p> </li> </ul>"},{"location":"fastapi/#ease-of-use","title":"Ease of Use","text":"<ul> <li> <p>Simple Syntax: FastAPI is known for its simple and intuitive syntax. Developers can quickly build APIs without extensive boilerplate code.</p> </li> <li> <p>Minimal Setup: Getting started with FastAPI is straightforward, with minimal setup required to create a fully functional API.</p> </li> <li> <p>Interactive Development: The framework allows for an interactive development experience with instant feedback, making it easier for developers to debug and iterate.</p> </li> </ul>"},{"location":"fastapi/#validation-and-serialization","title":"Validation and Serialization","text":"<ul> <li> <p>Pydantic Integration: FastAPI leverages Pydantic for data validation and serialization. This ensures that data is validated against the schema before processing, preventing many common bugs.</p> </li> <li> <p>Automatic Type Checking: By using Python type hints, FastAPI can automatically generate validation rules, ensuring that the data conforms to the expected types.</p> </li> <li> <p>Error Handling: FastAPI provides detailed error messages when validation fails, helping developers quickly identify and fix issues.</p> </li> </ul>"},{"location":"fastapi/#dependency-injection","title":"Dependency Injection","text":"<ul> <li> <p>Built-in Dependency Injection: FastAPI has built-in support for dependency injection, allowing developers to manage and inject dependencies into their functions easily.</p> </li> <li> <p>Modularity: This feature promotes modular and reusable code, as dependencies can be defined once and used across multiple endpoints.</p> </li> <li> <p>Scalability: With dependency injection, it\u2019s easier to scale applications by managing complex dependencies in a clear and organized manner.</p> </li> </ul>"},{"location":"fastapi/#auto-generated-documentation","title":"Auto-generated Documentation","text":"<ul> <li> <p>Interactive API Docs: FastAPI automatically generates interactive API documentation using Swagger UI and ReDoc. These docs allow developers and users to interact with the API directly from the browser.</p> </li> <li> <p>Self-updating: The documentation is always up-to-date with the code, thanks to FastAPI's reliance on Python type hints and Pydantic models.</p> </li> <li> <p>User-friendly: The auto-generated docs are highly user-friendly and provide comprehensive information about each endpoint, making it easier for developers to understand and use the API.</p> </li> </ul>"},{"location":"fastapi/#security-features","title":"Security Features","text":"<ul> <li> <p>OAuth2 and JWT: FastAPI has built-in support for OAuth2 and JSON Web Tokens (JWT), making it easier to implement secure authentication and authorization mechanisms.</p> </li> <li> <p>Security Dependencies: The framework provides security dependencies that can be used to enforce authorization rules, ensuring that only authorized users can access certain endpoints.</p> </li> <li> <p>HTTPS and CORS: FastAPI supports HTTPS and Cross-Origin Resource Sharing (CORS) out-of-the-box, enhancing the security of web applications.</p> </li> </ul>"},{"location":"fastapi/#key-points_1","title":"Key Points:","text":"<ul> <li>High performance due to asynchronous capabilities</li> <li>Simple and intuitive, quick development</li> <li>Automatic documentation with Swagger UI and ReDoc</li> <li>Robust data validation with Pydantic</li> <li>Built-in dependency injection and security</li> </ul>"},{"location":"fastapi/#architecture-of-fastapi","title":"Architecture of FastAPI","text":""},{"location":"fastapi/#components-overview","title":"Components Overview","text":"<ul> <li> <p>ASGI Server: Explain that FastAPI is built on the ASGI (Asynchronous Server Gateway Interface), which allows for async programming.</p> </li> <li> <p>Starlette: Describe how Starlette provides the web microframework capabilities.</p> </li> <li>Pydantic: Highlight how Pydantic is used for data validation and settings management.</li> <li>Routing: Explain the routing mechanism, how endpoints are defined using Python decorators.</li> </ul>"},{"location":"fastapi/#asgi-server","title":"ASGI Server","text":"<p>ASGI (Asynchronous Server Gateway Interface):</p> <ul> <li> <p>Description: ASGI is a specification that serves as a standard interface between asynchronous web servers and web applications or frameworks. It is designed to handle asynchronous programming, allowing for non-blocking operations and better performance.</p> </li> <li> <p>Role in FastAPI: FastAPI relies on ASGI to handle asynchronous requests, making it capable of high concurrency and efficient resource utilization. It enables FastAPI to support WebSockets, background tasks, and other async features seamlessly.</p> </li> <li> <p>Popular ASGI Servers: Examples include Uvicorn and Daphne, which can run ASGI applications like FastAPI.</p> </li> </ul>"},{"location":"fastapi/#starlette","title":"Starlette","text":"<p>Starlette Framework:</p> <ul> <li> <p>Description: Starlette is a lightweight ASGI framework/toolkit for building high-performance asynchronous web applications and services. It provides the core functionality for routing, middleware, and server-side components in FastAPI.</p> </li> <li> <p>Role in FastAPI: FastAPI is built on top of Starlette, leveraging its features for handling web requests, middleware, WebSocket support, and more. This integration ensures that FastAPI applications are both fast and scalable.</p> </li> <li> <p>Key Features: Starlette includes support for sessions, authentication, WebSockets, and background tasks, all of which enhance the capabilities of FastAPI.</p> </li> </ul>"},{"location":"fastapi/#pydantic","title":"Pydantic","text":"<p>Pydantic Library:</p> <ul> <li> <p>Description: Pydantic is a data validation and settings management library that uses Python type annotations. It ensures that data conforms to predefined schemas, providing automatic type checking and serialization.</p> </li> <li> <p>Role in FastAPI: FastAPI uses Pydantic for data validation, serialization, and documentation. Pydantic models are used to define request bodies, query parameters, and response schemas.</p> </li> <li> <p>Key Features: Pydantic provides robust error handling, detailed validation error messages, and support for complex data types, making it easier to handle and validate input data in FastAPI applications</p> </li> </ul> <pre><code>from pydantic import BaseModel\n\nclass Item(BaseModel):\n    name: str\n    price: float\n</code></pre>"},{"location":"fastapi/#routing","title":"Routing","text":"<p>Routing in FastAPI:</p> <ul> <li> <p>Description: Routing refers to the process of defining URL paths and associating them with specific handler functions that process requests and generate responses.</p> </li> <li> <p>Role in FastAPI: FastAPI uses a simple and intuitive decorator-based syntax for defining routes. This makes it easy to create and manage endpoints.</p> </li> <li> <p>Key Features: FastAPI supports path parameters, query parameters, and request body parsing through its routing system. It also provides automatic validation and documentation for defined routes.</p> </li> </ul> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int):\n    return {\"item_id\": item_id}\n</code></pre>"},{"location":"fastapi/#key-points_2","title":"Key Points","text":"<ul> <li>ASGI: Core of asynchronous capabilities</li> <li>Starlette: Foundation for web components</li> <li>Pydantic: Ensures robust data handling</li> <li>Routing: Simplified with decorators</li> </ul>"},{"location":"fastapi/#integration-and-extensions","title":"Integration and Extensions","text":""},{"location":"fastapi/#middleware","title":"Middleware","text":"<p>Allows you to run code before and after each request. It's useful for tasks like logging, authentication, and request/response transformation. To create middleware, you use the @app.middleware(\"http\") decorator on a function. </p> <p>from fastapi import FastAPI, Request import time</p> <pre><code>app = FastAPI()\n\n@app.middleware(\"http\")\nasync def add_process_time_header(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    return response\n</code></pre>"},{"location":"fastapi/#custom-components","title":"Custom Components","text":"<p>Allow you to extend the framework's functionality. This can include creating custom middleware, customizing the API documentation UI, or adding new features to your API</p> <pre><code>from starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import Response\n\nclass CustomMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        print(\"Before request\")\n        response = await call_next(request)\n        print(\"After request\")\n        return response\n</code></pre>"},{"location":"fastapi/#third-party-integrations","title":"Third-Party Integrations","text":"<p>FastAPI supports integration with third-party libraries and services. You can use any ASGI middleware that follows the ASGI specification.</p> <pre><code>from fastapi import FastAPI\nfrom unicorn import UnicornMiddleware\n\napp = FastAPI()\napp.add_middleware(UnicornMiddleware, some_config=\"rainbow\")\n</code></pre>"},{"location":"fastapi/#key-points_3","title":"Key Points:","text":"<ul> <li>Flexible middleware integration</li> <li>Extendable with custom components</li> <li>Easy integration with third-party tools</li> </ul>"},{"location":"fastapi/#benefits-and-limitations","title":"Benefits and Limitations","text":""},{"location":"fastapi/#benefits","title":"Benefits","text":"<ul> <li>High performance and efficient handling of concurrent requests.</li> <li>Ease of development with type hints and auto-generated docs.</li> <li>Strong community support and extensive documentation.</li> <li>Automatic interactive documentation enhances developer experience.</li> </ul>"},{"location":"fastapi/#limitations","title":"Limitations","text":"<ul> <li>Newer framework with a smaller ecosystem compared to Django or Flask.</li> <li>Requires understanding of asynchronous programming and type hints.</li> <li>May not be suitable for all types of applications, especially those not requiring high performance or asynchronous capabilities.</li> </ul>"},{"location":"grafana/","title":"Grafana","text":"<p>This part presents an overview of Grafana tool for educational purposes.</p>"},{"location":"grafana/#overview","title":"Overview","text":"CreatorUse Cases <ul> <li>Name: Torkel \u00d6degaard</li> <li>Background: Torkel \u00d6degaard is a software engineer and entrepreneur who founded Grafana Labs, the company behind Grafana</li> <li>Contribution: Torkel \u00d6degaard is the primary creator of Grafana, which started as a project to visualize time series data. His vision and leadership have driven the development of Grafana into a powerful, open-source platform used by millions worldwide for monitoring and visualization</li> </ul> <ul> <li>See Use Cases </li> </ul>"},{"location":"grafana/#overview_1","title":"Overview","text":"<p>Grafana is an open-source platform for monitoring, visualization, and alerting. It enables users to visualize time series data and other metrics from a variety of sources, offering powerful capabilities to create interactive and customizable dashboards.  Grafana is widely used in the industry to gain insights into system performance, application metrics, and business data.</p>"},{"location":"grafana/#key-features","title":"Key Features","text":""},{"location":"grafana/#data-source-integration","title":"Data Source Integration","text":""},{"location":"grafana/#support-for-multiple-data-sources","title":"Support for Multiple Data Sources","text":"<p>Grafana integrates with a wide range of data sources, allowing users to consolidate data from different systems into a single platform. Supported data sources include:</p> <ul> <li> <p>Prometheus: A popular monitoring system and time series database.</p> </li> <li> <p>InfluxDB: An open-source time series database designed for high-performance handling of time series data.</p> </li> <li> <p>Graphite: A monitoring tool that provides real-time visualization and storage of numeric time-series data.</p> </li> <li> <p>Elasticsearch: A distributed, RESTful search and analytics engine.</p> </li> <li> <p>AWS CloudWatch: A monitoring service for AWS cloud resources and applications.</p> </li> <li> <p>MySQL/PostgreSQL: Traditional relational databases that can be queried for metrics.</p> </li> </ul>"},{"location":"grafana/#query-editor","title":"Query Editor","text":"<p>Grafana offers query editors tailored for each data source, providing an intuitive interface for writing queries and visualizing data. </p> <p>These editors support features like syntax highlighting, auto-completion, and interactive query building, making it easier for users to craft and refine their queries.</p>"},{"location":"grafana/#dashboards-and-visualizations","title":"Dashboards and Visualizations","text":""},{"location":"grafana/#customizable-dashboards","title":"Customizable Dashboards","text":"<p>Grafana allows users to create and customize dashboards with a wide range of visualization options, such as graphs, tables, heatmaps, and more. </p> <p>Users can combine different panels to build comprehensive and meaningful dashboards that provide insights into their data.</p>"},{"location":"grafana/#reusable-panels","title":"Reusable Panels","text":"<p>Panels in Grafana can be reused across different dashboards, promoting consistency and efficiency. This feature allows users to create complex visualizations once and use them in multiple places, saving time and ensuring uniformity.</p>"},{"location":"grafana/#alerting","title":"Alerting","text":""},{"location":"grafana/#alert-rules","title":"Alert Rules","text":"<p>Users can define alert rules based on queries and set conditions to trigger alerts. Grafana evaluates these rules at regular intervals, and when conditions are met, it triggers the corresponding alerts. This helps in proactive monitoring and immediate response to potential issues.</p>"},{"location":"grafana/#notification-channels","title":"Notification Channels","text":"<p>Grafana supports various notification channels, enabling users to receive alerts through their preferred mediums. Supported channels include:</p> <ul> <li> <p>Email: Sends alerts via email.</p> </li> <li> <p>Slack: Integrates with Slack for real-time notifications.</p> </li> <li> <p>PagerDuty: Uses PagerDuty for incident management.</p> </li> <li> <p>Webhooks: Sends alerts to custom endpoints via webhooks.</p> </li> </ul>"},{"location":"grafana/#plugins-and-extensions","title":"Plugins and Extensions","text":""},{"location":"grafana/#plugins","title":"Plugins","text":"<p>Grafana has an extensive plugin ecosystem that extends its functionality. Plugins are available for:</p> <ul> <li> <p>Data Source Plugins: Connect Grafana to different data sources.</p> </li> <li> <p>Panel Plugins: Add new types of visualizations to dashboards.</p> </li> <li> <p>App Plugins: Provide full applications within Grafana, combining data sources, panels, and custom pages.</p> </li> </ul>"},{"location":"grafana/#community-and-enterprise-plugins","title":"Community and Enterprise Plugins","text":"<p>Grafana offers both community-contributed and enterprise-grade plugins. Community plugins are freely available and contributed by developers worldwide, while enterprise plugins offer additional features and support for Grafana Enterprise users.</p>"},{"location":"grafana/#annotations","title":"Annotations","text":""},{"location":"grafana/#annotations-on-graphs","title":"Annotations on Graphs","text":"<p>Users can add annotations to graphs to mark specific events or periods, providing context to the data visualizations. Annotations can be added manually or automatically based on query results, helping to highlight important events and patterns in the data.</p>"},{"location":"grafana/#user-management-and-security","title":"User Management and Security","text":""},{"location":"grafana/#user-authentication-and-authorization","title":"User Authentication and Authorization","text":"<p>Grafana supports various authentication methods, including LDAP, OAuth, and SAML, ensuring secure access to the platform.</p> <p>Fine-grained access control allows administrators to manage user permissions, ensuring that users have the appropriate level of access to dashboards and data sources.</p>"},{"location":"grafana/#use-cases","title":"Use Cases","text":"<ul> <li> <p>Infrastructure Monitoring: Monitor the performance and health of servers, networks, and applications.</p> </li> <li> <p>Application Performance Monitoring (APM): Track application metrics and performance indicators.</p> </li> <li> <p>Business Metrics: Visualize key business metrics to make data-driven decisions.</p> </li> <li> <p>IoT Monitoring: Monitor IoT devices and sensor data in real-time.</p> </li> </ul>"},{"location":"grafana/#grafana-architecture","title":"Grafana Architecture","text":""},{"location":"grafana/#core-components","title":"Core Components","text":""},{"location":"grafana/#frontend","title":"Frontend","text":"<ul> <li> <p>User Interface : Grafana provides a user-friendly and interactive web interface for creating and managing dashboards. The interface is designed to be intuitive, allowing users to easily navigate and configure their visualizations.</p> </li> <li> <p>React Framework : The frontend of Grafana is built using React, a popular JavaScript framework. This provides a responsive and dynamic user experience, enabling real-time updates and smooth interactions.</p> </li> </ul>"},{"location":"grafana/#backend","title":"Backend","text":"<ul> <li> <p>Go Language : The backend of Grafana is written in Go, a programming language known for its performance and concurrency capabilities. This ensures that Grafana can handle large volumes of data and multiple simultaneous users efficiently.</p> </li> <li> <p>API Endpoints : Grafana exposes RESTful API endpoints that enable interaction with the platform from external systems and scripts. These APIs allow for programmatic management of dashboards, data sources, and alerts.</p> </li> </ul>"},{"location":"grafana/#data-source-integrations","title":"Data Source Integrations","text":""},{"location":"grafana/#data-source-plugins","title":"Data Source Plugins","text":"<ul> <li> <p>Extensibility : Grafana enables to connect with a wide range of data storage systems, allowing users to aggregate and visualize data from multiple sources.</p> </li> <li> <p>Query Processing : Queries are processed and executed against the connected data sources ensuring that users have access to up-to-date and accurate data.ensuring that users have access to up-to-date and accurate data.</p> </li> </ul>"},{"location":"grafana/#storage-and-caching","title":"Storage and Caching","text":""},{"location":"grafana/#metadata-storage","title":"Metadata Storage","text":"<ul> <li> <p>Database Options: By default, Grafana uses SQLite to store metadata, such as dashboard configurations and user settings. For larger installations, users have the option to use MySQL or PostgreSQL, providing scalability and reliability.</p> </li> <li> <p>Configuration Storage: Dashboard configurations, user settings, and other metadata are stored and managed within the chosen database, ensuring consistency and persistence.</p> </li> </ul>"},{"location":"grafana/#caching","title":"Caching","text":"<ul> <li>Query Caching: Grafana employs caching mechanisms to optimize query performance and reduce load on data sources. Cached query results are stored and reused, minimizing the need to repeatedly execute the same queries.</li> </ul>"},{"location":"grafana/#visualization-engine","title":"Visualization Engine","text":""},{"location":"grafana/#rendering","title":"Rendering","text":"<ul> <li> <p>Dynamic Rendering: Grafana renders visualizations dynamically based on the data returned from queries. This ensures that the visualizations are always up-to-date and reflect the latest data.</p> </li> <li> <p>Canvas and SVG: Grafana uses canvas and SVG technologies for high-quality rendering of graphs and charts. These technologies provide flexibility and performance, enabling complex and detailed visualizations.</p> </li> </ul>"},{"location":"grafana/#alerting-engine","title":"Alerting Engine","text":""},{"location":"grafana/#alert-evaluator","title":"Alert Evaluator","text":"<ul> <li>Evaluation of Alert Rules: The alerting engine evaluates alert rules based on query results. When conditions specified in the alert rules are met, the engine triggers the corresponding alerts, enabling proactive monitoring and response.</li> </ul>"},{"location":"grafana/#notification-system","title":"Notification System","text":"<ul> <li>Notification Dispatch: Notifications are dispatched to various channels when alerts are triggered. This ensures that users are promptly informed of any issues, allowing for timely intervention and resolution.</li> </ul>"},{"location":"grafana/#security-and-authentication","title":"Security and Authentication","text":""},{"location":"grafana/#authentication-methods","title":"Authentication Methods","text":"<ul> <li> <p>Multiple Authentication Methods: Grafana supports multiple authentication methods, including LDAP, OAuth, and SAML. This ensures that access to the platform is secure and can be integrated with existing authentication systems.</p> </li> <li> <p>Role-Based Access Control (RBAC): Role-Based Access Control (RBAC) is implemented to manage user permissions and access levels. This allows administrators to define roles and assign permissions, ensuring that users have the appropriate level of access to the platform.</p> </li> </ul>"},{"location":"grafana/#plugins-and-extensions_1","title":"Plugins and Extensions","text":""},{"location":"grafana/#plugin-architecture","title":"Plugin Architecture","text":"<ul> <li> <p>Types of Plugins: Grafana supports different types of plugins, including data source plugins, panel plugins, and app plugins.</p> </li> <li> <p>Plugin Management: Users can browse available plugins, install them, and configure them through the Grafana interface, enhancing the platform's capabilities.</p> </li> </ul>"},{"location":"grafana/#use-cases-and-benefits","title":"Use Cases and Benefits","text":""},{"location":"grafana/#use-cases_1","title":"Use Cases","text":"<ul> <li> <p>Infrastructure Monitoring: Monitor the health and performance of servers, networks, and applications</p> </li> <li> <p>Application Performance Monitoring (APM): Track application metrics such as response times, error rates, and throughput.</p> </li> <li> <p>Business Metrics: Visualize key business metrics like sales figures, customer engagement, and financial performance.</p> </li> <li> <p>IoT Monitoring: Monitor data from IoT devices and sensors in real-time for insights and anomaly detection.</p> </li> </ul>"},{"location":"grafana/#benefits","title":"Benefits","text":"<ul> <li> <p>Versatility: Grafana's ability to integrate with numerous data sources makes it a versatile tool for various monitoring and visualization needs.</p> </li> <li> <p>Customization: Highly customizable dashboards and visualizations allow users to tailor Grafana to their specific requirements.</p> </li> <li> <p>Scalability: Suitable for both small setups and large-scale enterprise environments, ensuring it grows with the user's needs.</p> </li> <li> <p>Community and Support: A strong community and extensive documentation provide valuable resources for users, along with enterprise support options.</p> </li> </ul>"},{"location":"mlflow/","title":"MLFlow","text":"<p>This part presents an overview of MLflow tool for educational purposes.</p>"},{"location":"mlflow/#overview","title":"Overview","text":"CreatorUse Cases <ul> <li>Name: Developed by a team of engineers and data scientists at Databricks with significant contributions from the open-source community</li> <li>Background: Databricks is a company founded by the creators of Apache Spark, and it focuses on providing a unified analytics platform for big data and machine learning.</li> <li>Contribution: The team at Databricks developed MLflow to address the complexities of managing the machine learning lifecycle, from experimentation to deployment.</li> </ul> <ul> <li> <p>Experiment Tracking</p> <ul> <li> <p>Logging Parameters and Metrics: Track and compare different runs of experiments by logging parameters, metrics, and artifacts.</p> </li> <li> <p>Experiment Visualization: Use the MLflow UI to visualize and compare experiments, making it easier to identify the best-performing models.</p> </li> </ul> </li> <li> <p>Model Management</p> <ul> <li> <p>Model Registry: Manage different versions of models, including their lineage, versioning, aliasing, tagging, and annotations.</p> </li> <li> <p>Model Deployment: Deploy models to various environments, such as real-time serving through REST APIs and batch inference on Apache Spark.</p> </li> </ul> </li> <li> <p>Reproducibility</p> <ul> <li> <p>Packaging Code: Package data science work into reusable and reproducible formats using MLflow Projects.</p> </li> <li> <p>Reproducible Runs: Ensure that experiments can be reproduced and shared easily, promoting collaboration and verification of results.</p> </li> </ul> </li> <li> <p>Model Deployment</p> <ul> <li> <p>Standardized Packaging: Package machine learning models in a standard format that can be used in various downstream tools.</p> </li> <li> <p>Deployment to Different Environments: Deploy models to different environments, including Docker, Apache Spark, Azure ML, and AWS SageMaker.</p> </li> </ul> </li> <li> <p>Model Evaluation</p> <ul> <li> <p>Objective Comparison: Facilitate objective model comparison, whether working with traditional machine learning algorithms or advanced LLMs.</p> </li> <li> <p>Evaluation Tools: Provide tools for evaluating model performance and making informed decisions.</p> </li> </ul> </li> <li> <p>Structuring ML Projects</p> <ul> <li>MLflow Recipes: Offer recommendations for structuring ML projects to ensure functional end results optimized for real-world deployment scenarios.</li> </ul> </li> </ul>"},{"location":"mlflow/#main-characteristics-of-mlflow","title":"Main Characteristics of MLflow","text":""},{"location":"mlflow/#tracking","title":"Tracking","text":""},{"location":"mlflow/#logging","title":"Logging","text":"<p>MLflow Tracking provides a simple API and UI for logging various aspects of your machine learning process, such as parameters, metrics, code versions, and output files. </p> <p>This enables users to keep track of different experiments and their results, ensuring that the development and evaluation process is well-documented and reproducible.</p>"},{"location":"mlflow/#centralized-repository","title":"Centralized Repository","text":"<p>All logged data is stored in a centralized repository, which captures essential details such as parameters, metrics, artifacts, data, and environment configurations. </p> <p>This centralization allows teams to gain insights into their models' evolution over time and compare different runs effectively.</p>"},{"location":"mlflow/#model-registry","title":"Model Registry","text":""},{"location":"mlflow/#model-management","title":"Model Management","text":"<p>MLflow Model Registry helps in handling different versions of models, managing their current state (e.g., staging, production), and ensuring smooth productionization. </p> <p>It provides a comprehensive platform to manage the lifecycle of machine learning models.</p>"},{"location":"mlflow/#lifecycle-management","title":"Lifecycle Management","text":"<p>The model registry offers APIs and a user interface to manage the full lifecycle of MLflow Models, including model lineage, versioning, aliasing, tagging, and annotations. </p> <p>This promotes collaboration and efficient model management within teams.</p>"},{"location":"mlflow/#projects","title":"Projects","text":""},{"location":"mlflow/#packaging","title":"Packaging","text":"<p>MLflow Projects allow you to package your data science work in a reusable and reproducible format. </p> <p>A project is defined by a directory of files or a Git repository containing your code, dependencies, and entry points for running the code.</p>"},{"location":"mlflow/#reproducibility","title":"Reproducibility","text":"<p>By packaging your work as an MLflow Project, you ensure that experiments can be reproduced and shared easily. This is crucial for collaboration and for verifying the results of experiments.</p>"},{"location":"mlflow/#deployments","title":"Deployments","text":""},{"location":"mlflow/#standardization","title":"Standardization","text":"<p>MLflow provides a standard format for packaging machine learning models, enabling them to be used in various downstream tools and deployment scenarios.</p>"},{"location":"mlflow/#deployment","title":"Deployment","text":"<p>MLflow supports deploying models to different environments, including real-time serving through REST APIs and batch inference on Apache Spark. </p> <p>This versatility ensures that models can be deployed and scaled effectively across different platforms.</p>"},{"location":"mlflow/#evaluate","title":"Evaluate","text":""},{"location":"mlflow/#model-analysis","title":"Model Analysis","text":"<p>MLflow Evaluate facilitates objective model comparison, whether you're working with traditional machine learning algorithms or advanced LLMs (large language models). It provides tools for evaluating model performance and making informed decisions.</p>"},{"location":"mlflow/#evaluation-tools","title":"Evaluation Tools","text":"<p>The evaluation tools in MLflow help assess model accuracy, precision, recall, and other performance metrics, ensuring that the best models are selected for deployment.</p>"},{"location":"mlflow/#recipes","title":"Recipes","text":""},{"location":"mlflow/#guidance","title":"Guidance","text":"<p>MLflow Recipes offers recommendations for structuring ML projects, ensuring that they are functional and optimized for real-world deployment scenarios. </p> <p>This guidance helps maintain consistency and best practices across different projects.</p>"},{"location":"mlflow/#mlflow-architecture","title":"MLflow Architecture","text":""},{"location":"mlflow/#mlflow-tracking","title":"MLflow Tracking","text":""},{"location":"mlflow/#core-component","title":"Core Component","text":"<p>The MLflow Tracking component is the core of the MLflow architecture. It logs, organizes, and visualizes machine learning experiments, providing a comprehensive view of all runs and their results.</p>"},{"location":"mlflow/#experiments-and-runs","title":"Experiments and Runs","text":"<p>MLflow organizes machine learning projects into experiments, which are groups of related runs. </p> <p>Each run captures the details of a single execution of the project, including parameters, metrics, and artifacts.</p>"},{"location":"mlflow/#artifacts-store","title":"Artifacts Store","text":"<p>The artifacts store is where all outputs of the experiments, such as models and figures, are stored. </p> <p>This ensures traceability and easy access to all related files.</p>"},{"location":"mlflow/#metrics-and-parameters","title":"Metrics and Parameters","text":"<p>MLflow logs metrics and parameters for each run, allowing users to compare different runs and optimize their models.</p>"},{"location":"mlflow/#dependencies-and-environment","title":"Dependencies and Environment","text":"<p>MLflow captures the computational environment, including dependencies and configurations, to ensure reproducibility of the experiments.</p>"},{"location":"mlflow/#ui-integration","title":"UI Integration","text":"<p>The integrated UI allows users to visualize and compare runs, facilitating better decision-making and analysis.</p>"},{"location":"mlflow/#apis","title":"APIs","text":"<p>MLflow provides APIs for interacting programmatically with the tracking system, enabling seamless integration with other tools and workflows.</p>"},{"location":"mlflow/#tracking-server","title":"tracking Server","text":"<p>MLflow Tracking Server can be configured with an artifacts HTTP proxy, passing artifact requests through the tracking server to store and retrieve artifacts without having to interact with underlying object store services. This is particularly useful for team development scenarios where you want to store artifacts and experiment metadata in a shared location with proper access control.</p> <p>Here a scheme of MLFflow Tracking in Team development as we used it :</p> <p></p>"},{"location":"mlflow/#mlflow-projects","title":"MLflow Projects","text":""},{"location":"mlflow/#project-definition","title":"Project Definition","text":"<p>An MLflow Project is defined by a directory of files or a Git repository that contains the project code. This makes it easy to share and reproduce the project.</p>"},{"location":"mlflow/#entry-points","title":"Entry Points","text":"<p>Projects include entry points, which are commands that can be executed within the project, along with their parameters. This makes running experiments straightforward and standardized.</p>"},{"location":"mlflow/#environment-specification","title":"Environment Specification","text":"<p>MLflow Projects specify execution environments, including Conda environments and Docker files. This ensures that the code runs in the correct environment, further promoting reproducibility.</p>"},{"location":"mlflow/#mlflow-models","title":"MLflow Models","text":""},{"location":"mlflow/#standard-format","title":"Standard Format","text":"<p>MLflow Models are packaged in a standard format that includes metadata, such as dependencies and inference schema. This standardization makes it easy to deploy models across different platforms.</p>"},{"location":"mlflow/#flavors","title":"Flavors","text":"<p>MLflow Models support multiple formats, known as flavors, making them compatible with various downstream tools and environments. For example, a model can be saved in a format that is compatible with TensorFlow, PyTorch, or scikit-learn.</p>"},{"location":"mlflow/#mlflow-deployments","title":"MLflow Deployments","text":""},{"location":"mlflow/#unified-interface","title":"Unified Interface","text":"<p>MLflow provides a unified interface for deploying models to various platforms, including SaaS and open-source solutions. This simplifies the deployment process and ensures consistency.</p>"},{"location":"mlflow/#security","title":"Security","text":"<p>MLflow bolsters security through authenticated access to models, ensuring that only authorized users can deploy and manage models.</p>"},{"location":"mlflow/#apis_1","title":"APIs","text":"<p>The deployment APIs provide a common set of functions for deploying models to different environments, making the process seamless and efficient.</p>"},{"location":"mlflow/#use-cases-and-benefits","title":"Use Cases and Benefits","text":""},{"location":"mlflow/#use-cases","title":"Use cases","text":"<ul> <li> <p>Monitoring Use Cases</p> <ul> <li> <p>Web Servers: Monitor HTTP request rates, response times, and error rates to ensure the performance and reliability of web applications.</p> </li> <li> <p>Databases: Track query performance, connection counts, and resource usage for databases like PostgreSQL, MySQL, and MongoDB.</p> </li> <li> <p>Kubernetes Clusters: Observe pod health, resource utilization, and cluster events to maintain the health and efficiency of Kubernetes clusters.</p> </li> </ul> </li> </ul> <p>For more use cases see Overview </p>"},{"location":"mlflow/#benefits","title":"Benefits","text":"<ul> <li> <p>Reliability: Prometheus is designed for high availability and resilience, ensuring that monitoring and alerting are reliable.</p> </li> <li> <p>Scalability: Prometheus handles large volumes of metrics data and scales with the infrastructure, making it suitable for growing environments.</p> </li> <li> <p>Ease of Integration: Prometheus seamlessly integrates with a wide range of third-party tools and services, providing a flexible and powerful monitoring solution.</p> </li> </ul>"},{"location":"model-ltsm/","title":"LTSM","text":""},{"location":"model-ltsm/#what-is-lstm","title":"What is LSTM","text":"<p>LSTM is a type of Recurrent Neural Network (RNN) designed to handle sequential data. Unlike traditional neural networks, LSTMs have a special architecture that allows them to remember information for long periods, making them particularly useful for tasks like time series forecasting, speech recognition, and natural language processing.</p>"},{"location":"model-ltsm/#lstm-within-the-machine-learning-and-deep-learning-ecosystem","title":"LSTM within the Machine Learning and Deep Learning Ecosystem","text":""},{"location":"model-ltsm/#machine-learning-ecosystem","title":"Machine Learning Ecosystem","text":"<p>The machine learning ecosystem includes a variety of models and techniques for analyzing data and making predictions. Some key components include:</p> <ul> <li> <p>Supervised Learning: Algorithms that learn from labeled data (e.g., Linear Regression, Decision Trees).</p> </li> <li> <p>Unsupervised Learning: Algorithms that find patterns in data without labeled responses (e.g., K-Means Clustering, PCA).</p> </li> <li> <p>Reinforcement Learning: Algorithms that learn by interacting with an environment and receiving feedback (e.g., Q-Learning).</p> </li> </ul>"},{"location":"model-ltsm/#deep-learning-ecosystem","title":"Deep Learning Ecosystem","text":"<p>Within the broader machine learning ecosystem, deep learning represents a subset that focuses on neural networks with multiple layers. </p> <p>Key components include:</p> <ul> <li> <p>Feedforward Neural Networks (FNNs): Basic neural networks where information moves in one direction, from input to output.</p> </li> <li> <p>Convolutional Neural Networks (CNNs): Specialized for processing grid-like data, such as images.</p> </li> <li> <p>Recurrent Neural Networks (RNNs): Designed for sequential data, such as time series or language.</p> </li> </ul>"},{"location":"model-ltsm/#position-of-lstm-within-deep-learning","title":"Position of LSTM within Deep Learning","text":"<p>Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN). They are designed to address the limitations of traditional RNNs, specifically their difficulty in learning long-term dependencies.</p>"},{"location":"model-ltsm/#characteristics-of-lstm","title":"Characteristics of LSTM:","text":"<ul> <li> <p>Memory Cells: LSTMs have memory cells that can store information over long periods.</p> </li> <li> <p>Gates: They use gates (input, forget, and output gates) to control the flow of information.</p> </li> </ul>"},{"location":"model-ltsm/#role-in-time-series-and-sequential-data","title":"Role in Time Series and Sequential Data:","text":"<p>LSTMs are particularly effective for tasks where the sequence of data points matters. Examples include:</p> <ul> <li> <p>Time Series Forecasting: Predicting future values based on past observations.</p> </li> <li> <p>Speech Recognition: Understanding spoken language by analyzing sequential audio data.</p> </li> <li> <p>Natural Language Processing (NLP): Tasks like text generation, translation, and sentiment analysis.</p> </li> </ul>"},{"location":"model-ltsm/#how-does-lstm-work","title":"How does LSTM work","text":"<p>Traditional RNNs face challenges with long-term dependencies because they suffer from the vanishing gradient problem. During training, the gradients (used to update the network weights) can become very small, effectively \"forgetting\" earlier data in the sequence. This limitation makes it hard for RNNs to learn dependencies that span over long sequences.</p> Go deeper <p>See more A Beginner's Guide to LSTMs and Recurrent Neural Networks</p>"},{"location":"model-ltsm/#problem-with-traditional-rnns","title":"Problem with Traditional RNNs","text":"<p>Traditional RNNs face challenges with long-term dependencies because they suffer from the vanishing gradient problem. During training, the gradients (used to update the network weights) can become very small, effectively \"forgetting\" earlier data in the sequence. </p> <p>This limitation makes it hard for RNNs to learn dependencies that span over long sequences.</p>"},{"location":"model-ltsm/#lstm-architecture","title":"LSTM Architecture","text":"<p>LSTMs address the limitations of traditional RNNs with a more complex architecture, featuring a set of gates that control the flow of information. </p> <p>LSTMs have a unique structure with gates that control the flow of information. These gates are:</p>"},{"location":"model-ltsm/#cell-state-c_t","title":"Cell State \\( (C_t) \\)","text":"<p>The cell state is the memory of the network. It carries information across different time steps, allowing the LSTM to remember important details over long sequences.</p>"},{"location":"model-ltsm/#gates","title":"Gates","text":"<p>LSTMs have three types of gates that regulate the cell state:</p>"},{"location":"model-ltsm/#forget-gate-c_t","title":"Forget Gate \\( (C_t) \\)","text":"<pre><code>* Decides what information to discard from the cell state.\n</code></pre>"},{"location":"model-ltsm/#input-gate-i_t","title":"Input Gate \\( (i_t) \\)","text":"<pre><code>* Determines which values from the input to update the cell state.\n</code></pre>"},{"location":"model-ltsm/#output-gate-o_t","title":"Output Gate \\( (O_t) \\)","text":"<pre><code>* Controls what part of the cell state to output.\n</code></pre>"},{"location":"model-ltsm/#the-flow-of-information-in-lstm","title":"The Flow of Information in LSTM","text":""},{"location":"model-ltsm/#forget-gate","title":"Forget Gate","text":"<p>At each time step \\( t \\), the forget gate determines what fraction of the previous cell state (\\( C_{t-1} \\)) to forget based on the current input (\\( x_t \\)) and previous hidden state (\\( h_{t-1} \\)):</p> \\[ f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\] <p>Where \\( \\sigma \\) is the sigmoid function, and \\( W_f \\)  and \\( b_f \\) are the weights and biases of the forget gate.</p> What does it mean ?How It WorksIn Simple TermsWhy It Matters <p>\\( f_t \\) : This is the output of the forget gate at time step. It decides how much information from the previous cell state should be forgotten.</p> <p>\\( \\sigma \\) : This is the sigmoid activation function. It squashes its input to be between 0 and 1. Essentially, it acts like a gate that decides what fraction of the information to pass through.</p> <p>\\( W_f \\) : These are the weights associated with the forget gate. They determine the importance of each input feature.</p> <p>\\( h_{t-1} \\) : This is the hidden state from the previous time step. It contains information from the past time steps.</p> <p>\\( x_t \\) : This is the input at the current time step. It contains the current data point or feature being processed.</p> <p>\\( b_f \\) : This is the bias term associated with the forget gate. It helps to adjust the output of the gate.</p> <p>1. Inputs to the Forget Gate:</p> <ul> <li>The hidden state from the previous time step \\( h_{t-1} \\) and the current input \\( x_t \\) are combined. </li> <li>This combination is essentially a way to capture both past information and current information.</li> </ul> <p>2. Weighted Sum:</p> <ul> <li>The combined input is then multiplied by the weight matrix \\( W_f \\), which helps to scale the input features based on their importance.</li> </ul> <p>3. Adding Bias:</p> <ul> <li>The bias \\( b_f \\) is added to the result. </li> <li>The bias ensures that the gate can operate even when the input values are zero.</li> </ul> <p>4. Sigmoid Activation:</p> <ul> <li>Finally, the sigmoid function \\( \\sigma \\) is applied. </li> <li>The sigmoid function squashes the result to be between 0 and 1. </li> <li>This squashing is crucial because it tells the model how much of the previous cell state should be \"forgotten.\" </li> <li>A value close to 0 means \"forget everything,\" and a value close to 1 means \"keep everything.\"</li> </ul> <ul> <li> <p>Imagine you are trying to remember something important for your tasks over a week. Each day (time step), you decide how much of the previous day's information to keep (hidden state) and how much of today's new information (current input) to consider. The forget gate helps you decide how much of the past information you should retain or discard, based on the relevance of the new information you are receiving.</p> </li> <li> <p>The combination of past information and current input, adjusted by weights and biases, goes through a \"decision function\" (sigmoid) that outputs a value between 0 and 1. This value guides you on how much of the past information to keep and how much to forget.</p> </li> </ul> <ul> <li>The forget gate is crucial because it helps the LSTM model manage long-term dependencies. </li> <li>By selectively forgetting irrelevant information, the model can focus on the more pertinent details, thereby improving its performance on tasks involving sequential data, like time series forecasting and language modeling.</li> </ul>"},{"location":"model-ltsm/#input-gate","title":"Input Gate","text":"<p>The input gate decides what new information to add to the cell state.  It consists of two parts:</p>"},{"location":"model-ltsm/#the-update-candidate","title":"The update candidate","text":"\\[ \\tilde{C_t} = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\]"},{"location":"model-ltsm/#the-gate-itself","title":"The gate itself","text":"\\[ i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\]"},{"location":"model-ltsm/#update-cell-state","title":"Update Cell State","text":"<p>The new cell state is a combination of the old cell state (modified by the forget gate) and the new candidate (modulated by the input gate):</p> \\[ C_t = f_t * C_{t-1} + i_t * \\tilde{C_t} \\] What does it mean ?How It WorksIn Simple TermsWhy It Matters <p>\\( \\tilde C \\): This is the new candidate value for the cell state. It represents potential new information that could be added to the cell state.</p> <p>\\( i_t \\): This is the output of the input gate at time step \\( t \\). It decides how much of the new candidate value should be added to the cell state.</p> <p>\\( \\tanh \\): This is the hyperbolic tangent activation function. It squashes its input to be between -1 and 1, helping to ensure the values stay within a reasonable range.</p> <p>\\( \\sigma \\): This is the sigmoid activation function. It squashes its input to be between 0 and 1.</p> <p>\\( W_c \\) and \\( W_i \\): These are the weights associated with the update candidate and input gate, respectively. They determine the importance of each input feature.</p> <p>\\( h_{t-1} \\): This is the hidden state from the previous time step. It contains information from past time steps.</p> <p>\\( Xt \\): This is the input at the current time step  \\( t \\). It contains the current data point or feature being processed.</p> <p>\\( b_c \\) and \\( b_i \\): These are the biases associated with the update candidate and input gate, respectively. They help adjust the output.</p> <p>1. Update Candidate:</p> <pre><code>* The update candidate \\( \\tilde C \\) is calculated by combining the hidden state from the previous time step \\( h_{t-1} \\) and the current input \\( Xt \\). \n* This combination is multiplied by the weight matrix \\( W_c \\) and added to the bias \\( b_c \\). \n* The result is then passed through the \\( \\tanh \\) function to create the new candidate value (see formula).\n</code></pre> <p>2. Input Gate:</p> <pre><code>* The input gate \\( i_t \\) determines how much of the new candidate value should be added to the cell state. \n* It combines the hidden state from the previous time step \\( h_{t-1} \\) and the current input \\( Xt \\), multiplies by the weight matrix \\( W_i \\), and adds the bias \\( b_i \\).\n* This result is passed through the \\( \\sigma \\) function to output a value between 0 and 1, indicating the proportion of the candidate value to be added to the cell state (see formula).\n</code></pre> <ul> <li>Imagine you're trying to learn a new skill, like playing the piano. Every day, you practice (current input \\( Xt \\) ) and recall what you learned previously (hidden state \\( h_{t-1} \\) ). The input gate helps you decide how much of the new practice (update candidate \\( \\tilde C \\) ) should be added to your overall skill level (cell state).</li> </ul> <p>1. Update Candidate: Think of this as creating a draft of what new information (new practice) could potentially be added to your memory.</p> <p>2. Input Gate: This acts like a filter, deciding how much of this new draft should actually be kept and added to your overall skill level.</p> <ul> <li> <p>The input gate is crucial because it determines how much new information gets incorporated into the memory of the LSTM. By carefully regulating the flow of new information, the LSTM can effectively learn and adapt to complex patterns in sequential data.</p> </li> <li> <p>This gating mechanism allows LSTMs to handle long-term dependencies better than traditional RNNs, making them more effective for tasks such as time series forecasting, language modeling, and speech recognition.</p> </li> </ul>"},{"location":"model-ltsm/#output-gate","title":"Output Gate","text":"<p>Finally, the output gate decides what to output based on the cell state and the input. The hidden state \\( h_t \\) is updated:</p> \\[ o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\] \\[ h_t = o_t * \\tanh(C_t) \\] What does it mean ?How It WorksIn Simple TermsWhy It Matters <p>\\( Xt \\): This is the output of the output gate at time step \\( t \\). It decides which parts of the cell state should be outputted.</p> <p>\\( h_t \\): This is the hidden state at time step \\( t \\). It is the information that gets passed to the next time step and is used for making predictions or further processing.</p> <p>\\( \\sigma \\): This is the sigmoid activation function. It squashes its input to be between 0 and 1.</p> <p>\\( \\tanh \\): This is the hyperbolic tangent activation function. It squashes its input to be between -1 and 1.</p> <p>\\( W_0 \\): These are the weights associated with the output gate. They determine the importance of each input feature.</p> <p>\\( h_{t-1} \\): This is the hidden state from the previous time step. It contains information from past time steps.</p> <p>\\( W_t \\): This is the input at the current time step \\( t \\). It contains the current data point or feature being processed.</p> <p>\\( b_0 \\): This is the bias term associated with the output gate. It helps to adjust the output of the gate.</p> <p>\\( C_t \\): This is the cell state at time step \\( t \\). It contains the memory of the network.</p> <p>1. Output Gate: </p> <pre><code>* The output gate \\( O_t \\) decides what part of the cell state should be outputted. It does this by combining the hidden state from the previous time step \\( h_{t-1} \\) and the current input \\( X_t \\). \n* This combination is multiplied by the weight matrix \\( W_O \\) and added to the bias \\( b_0 \\).\n* The result is then passed through the sigmoid function \\( \\sigma \\), which outputs a value between 0 and 1 (see formula).\n</code></pre> <p>2. Hidden State Update:</p> <pre><code>* The hidden state \\( h_t \\) is updated by taking the output of the output gate \\( O_t \\) and multiplying it element-wise with the hyperbolic tangent of the cell state \\( C_t \\)  (see formula).\n</code></pre> <ul> <li>Imagine you are working on a project, and each day you decide what information from your notes (cell state) you will use (output) for the next task (hidden state). </li> <li>The output gate helps you make this decision.</li> </ul> <p>-1. Output Gate:_</p> <pre><code>* Think of this as a filter that decides which parts of your notes are relevant for the task at hand. It looks at what you learned previously (hidden state) and what you currently need (input).\n</code></pre> <p>2. Hidden State Update:</p> <pre><code>* Once the output gate has decided what to use, it combines this filtered information with the current state of your memory (cell state) to update your working notes (hidden state) for the next task.\n</code></pre> <ul> <li>The output gate is crucial because it determines what information is passed on to the next time step and what is outputted by the LSTM. </li> <li>By controlling the flow of information, the output gate ensures that only the most relevant information is used for making predictions or further processing. </li> <li>This makes LSTMs effective for tasks like time series forecasting, language modeling, and speech recognition.</li> <li>By filtering and updating the hidden state, LSTMs can maintain and utilize long-term dependencies in sequential data, which is essential for understanding complex patterns over time.</li> </ul>"},{"location":"model-ltsm/#characteristics-and-benefits","title":"Characteristics and Benefits","text":"<ul> <li> <p>Long-Term Dependencies: LSTMs can capture and utilize long-term dependencies in sequential data, thanks to their memory cells and gating mechanisms.</p> </li> <li> <p>Versatility: Suitable for various domains where sequential data is crucial, such as finance (time series forecasting), healthcare (patient monitoring), and NLP (language modeling).</p> </li> <li> <p>Efficiency: Although computationally intensive, LSTMs can learn complex patterns in data that other models might miss.</p> </li> </ul>"},{"location":"model-ltsm/#limitations","title":"Limitations","text":"<ul> <li> <p>Complexity: The architecture of LSTMs is more complex compared to traditional RNNs, requiring more computational resources.</p> </li> <li> <p>Training Time: LSTMs typically take longer to train due to their complexity and the need to backpropagate through time.</p> </li> <li> <p>Sensitivity to Hyperparameters: Performance can be significantly affected by the choice of hyperparameters, requiring careful tuning.</p> </li> </ul>"},{"location":"modeling/","title":"Modeling","text":""},{"location":"modeling/#introduction","title":"Introduction","text":""},{"location":"modeling/#bitcoin","title":"Bitcoin","text":"<p>Bitcoin is an electronic currency introduced in 2008 by Nakamoto. It was made possible by blockchain technology and allows for secure peer-to-peer transactions using cryptography.</p> <p>Today, bitcoin (BTC) is the most important and widely used cryptocurrency. It is used in many financial and commercial activities.</p> <p>Predicting the prices of cryptocurrencies is crucial for investors and academics in this field due to the frequent volatility of the price of this currency.</p> <p>Predicting the price of cryptocurrencies is a time series prediction problem. The time step chosen for the project is a one-day interval for short-term prediction of the price.</p>"},{"location":"modeling/#financial-cryptocurrency-market","title":"financial cryptocurrency market","text":"<p>The financial cryptocurrency market, similar to traditional stock exchanges, operates through the buying and selling of digital assets. </p> <p>Unlike stock exchanges where assets are shares in companies, cryptocurrencies are digital tokens that utilize blockchain technology. </p> <p>Bitcoin, the first and most well-known cryptocurrency, has unique characteristics such as high volatility and 24/7 trading availability. </p> <p>Predicting Bitcoin prices is crucial for financial strategy and profitability as it helps investors make informed decisions. </p>"},{"location":"modeling/#price-prediction-of-cryptocurrencies","title":"Price prediction of cryptocurrencies","text":"<p>In the literature on time series prediction, Machine Learning (ML) algorithms have been widely used. Due to the extreme volatility and lack of seasonality in the Bitcoin market, linear price prediction methods have proven to be unsuccessful.</p> <p>Scientific studies have shown that Deep Learning models (Recurrent Neural Networks (RNN), Gradient Boosting Classifiers (GBC)) are particularly well-suited for short-term prediction of the Bitcoin market behavior, and that input technical variables such as past price, the evolution of Bitcoin returns over time, and characteristics of its blockchain (such as the number of BTC transactions) have the most significant impact on short-term prediction.</p> <p>Other predictive variables such as the price of the MSCI World index, the S&amp;P 500, the Nasdaq, the price of gold or a barrel of oil can also be used.</p> <p>Several studies have concluded that the LSTM model is the most performant in terms of prediction with metrics such as Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared score (R2) (exemple)</p> <p>Other studies have introduced sentiment variables of its community through sentiment analysis (identification and classification of sentiments expressed in a natural language text.Philippas et al.(2019), using tools such as VADER or TextBlob, or machine learning models to analyze the sentiment of tweets) supporting the idea that the price of Bitcoin is determined by the sentiment of the Bitcoin community on social networks. These variables seem to gain importance for the medium and long-term prediction of Bitcoin. Philippas et al.(2019)</p> <p>Sources such as Twitter (API Twitter: tweets containing relevant keywords such as \"Bitcoin\", \"BTC\", \"crypto\", etc.) or Google Trends (Google Trend API) have been used (search trends as an indicator of community interest and engagement).</p>"},{"location":"modeling/#model-and-predictive-variables","title":"Model and Predictive Variables","text":"<p>The model considered for this project is a Long Short-Term Memory (LSTM) recurrent neural network. The predictive variable used will be the past price of Bitcoin (daily closing price), which may be optionally associated with other technical variables such as transaction volume. The temporality and objective of the project are such that sentiment analysis and trend analysis will not be implemented, especially since it is a one-day prediction and that it is not the aim of this MLops project.</p>"},{"location":"modeling/#methodology","title":"Methodology","text":""},{"location":"modeling/#introduction_1","title":"Introduction","text":"<p>Our standard methodology involves several key steps:  - data collection - data loading and preprocessing - model creation - model training - model prediction and evaluation - model deployment. </p> <p>The data is preprocessed, normalized, and split into training and test sets.</p> <p>An LSTM model is developed, trained, and evaluated using MLflow for tracking and versioning. </p> <p>The final model is deployed via FastAPI, with Prometheus and Grafana for monitoring and logging. Streamlit is used to create a user-friendly dashboard for visualizing predictions.</p>"},{"location":"modeling/#structure","title":"Structure","text":"<pre><code>Main Repo/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\u2502   \n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\u2502\n\u2502   \u2502   \u251c\u2500\u2500 database.py  # crypto Database Connection\n\u2502   \u2502   \u251c\u2500\u2500 import_raw_data.py  # Loading data\n\u2502   \u2502   \u251c\u2500\u2500 make_dataset.py  # Construction of appropriate Dataset for LSTM model\n\u2502   \u251c\u2500\u2500 evaluation/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\u2502\n\u2502   \u2502   \u251c\u2500\u2500 evaluate.py  # Scaling and score modules\n\u2502   \u2502   \u251c\u2500\u2500 ml_flow.py  # modules to initiate, get, lmoad experiments and models in mlflow\n\u2502   \u251c\u2500\u2500 features/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\u2502\n\u2502   \u2502   \u251c\u2500\u2500 preprocess.py  # Normalization of Data\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\u2502\n\u2502   \u2502   \u251c\u2500\u2500 modelLSTM.py  # CLass LSTM for modelling\n\u2502   \u2502   \u251c\u2500\u2500 train_model M.py  # Callbacks and train module\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 __init__.py\u2502\n\u2502   \u251c\u2500\u2500 train2.py    # Main Training script\n\u2502   \u251c\u2500\u2500 predict2.py   # Main predicting script\n\u2502   \u251c\u2500\u2500 evaluate_model2.py # Main evaluation script\u2502\n\u2502   \u251c\u2500\u2500 Dockerfile  # Docker configuration file for modelling (airflow worker)\n\n## Flow and sequencing overview\n\n### Data loading and transforming\n\nWe used Airflow to automate data collection from a postgres SQL Database named (data_db) filled with data from Data provider Kraken,   throught Python module *import_raw_data*\nFirst, useless columns from the ohlv table in data_db are removed through function *normalize_data2*\nThen numerical features are beeing normalized to ensure that all features contribute equally to the model training process. Technique Min-Max scaling was used.\n\nWe used Airflow to download full historical data for each cryptocurrency pair which is essential to capture all relevant patterns. Hourly or daily automated data retrieval ensures the dataset is up-to-date and comprehensive, depending on the prediction step chosen.\nOne day was chosen for the project, but it would be possible to use the project with one minute, one hour, one week predicion interval.\n\n### Predictive variable\n\n#### Mechanism of cryptocurrency pricing \n\nThe mechanism of cryptocurrency pricing is driven by market demand and supply dynamics, influenced by factors such as market sentiment, economic news, and regulatory developments. Data can be formatted using the Open-High-Low-Close (OHLC) standard, which captures key price points for each time interval. Below is an example of how to retrieve data using Python from Kaggle:\n\n```python\nprint('hello world')\n</code></pre>"},{"location":"modeling/#decision","title":"Decision","text":"<p>We decided to retain only the cryptocurrency past price to predict cryptocurrency future price. This choice has been made taking incto account the short term prediction chosen (1 day) and conclusions of several scientific bibliography on the topic.</p>"},{"location":"modeling/#building-dataset-for-training-and-testing","title":"Building dataset for training and testing","text":"<p>Transformed time-series data was splited into 2 datasets with 70% training data and 30% testing data. Then Sequencing of time series was made using two differents periods of time for training dataset and testing dataset as illustrated in the following graph :</p> <p></p> <p>Two sequences was built (X and y) through Python module make_dataset: - X is a sequence of prices of size pas_temps (parameter representing the number of past days which are used directly for prediction) - y is the next value, which will be the value to predict.</p> <p>Parameter pas_temps is an important parameter of the model. This paramater has been finally fixed to 14 days and not used as a parameter in order to reduce training time and model complexity.This value has been chosen after optimization tests. </p>"},{"location":"modeling/#modelling","title":"Modelling","text":""},{"location":"modeling/#model-selection","title":"Model selection","text":"<p>A simple LSTM model was chosen taking into account scientific publications and its good performance in time series crypty prediction.</p> <p>Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to capture long-term dependencies in sequential data. LSTMs use memory cells to maintain information over time, which helps in learning time-series patterns more effectively. </p> <p>Below is a LaTeX representation of an LSTM cell:</p>"},{"location":"modeling/#lstm-model-architecture","title":"LSTM Model Architecture","text":"<p>[explain with mermaid diagram the whole LTSM architecture in detail with explaination]</p> <p>this graph must be 're modeling' ! </p> <pre><code>graph TD;\n    Input--&gt;LSTM_Cell_1;\n    LSTM_Cell_1--&gt;LSTM_Cell_2;\n    LSTM_Cell_2--&gt;LSTM_Cell_3;\n    LSTM_Cell_3--&gt;Output;\n\n    subgraph LSTM_Cell_1\n    direction TB\n    Forget_Gate--&gt;Memory_Cell\n    Input_Gate--&gt;Memory_Cell\n    Output_Gate--&gt;Memory_Cell\n    end\n\n    subgraph LSTM_Cell_2\n    direction TB\n    Forget_Gate--&gt;Memory_Cell\n    Input_Gate--&gt;Memory_Cell\n    Output_Gate--&gt;Memory_Cell\n    end\n\n    subgraph LSTM_Cell_3\n    direction TB\n    Forget_Gate--&gt;Memory_Cell\n    Input_Gate--&gt;Memory_Cell\n    Output_Gate--&gt;Memory_Cell\n    end</code></pre> <p>The architecture consists of multiple LSTM cells that process input sequentially, allowing the model to learn temporal dependencies.</p>"},{"location":"modeling/#model-creation-and-instanciation","title":"Model creation and instanciation","text":"<p>A custom Class LSTMModel is created to instanciate the model, with the following parameters :     -neurons: number of neurons in the network;     -loss function used during training: mean squarred error ('mse')     -metrics: 'mse' metrics, used in mlflow used to differentiate models performance.     -optimizer used during compiling: 'adam'     -activation function: \"relu\"</p> <p>An LSTM like Model was created using a Sequential object, composed of an input layer (default size (None,1)), an LSTM layer and a Dense layer with 1 neuron for prediction. LSTM Class is composed of following built-in methods : init, fit, predict, save_model, load_model, get_params, set_params and str</p>"},{"location":"modeling/#model-training","title":"Model Training","text":""},{"location":"modeling/#mlflow","title":"MLflow","text":"<p>Model training has been realized with MLflow.</p> <p>MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It was developed by Databricks to address the complexities of model tracking, versioning, and deployment. MLflow works by logging metrics, parameters, and artifacts during training. These logs can then be used to compare different runs and select the best model for deployment.</p>"},{"location":"modeling/#practical-implementation","title":"Practical implementation","text":"<p>The model has been trained with training dataset. In ths context, Callbacks early_stopping and reduce_learning_rate has been used during training to make it more fast and efficient. Loss function fastly reach its equilibium level as seen on this graph :</p> <p></p> <p>An experiment is created in MLflow Tracking and an mlflow run is created for each parameter tested (pas_temps, batch_size or neurons) of the LTSM model. Then model score and parameters are logged into mlflow tracking Server. Finaly, the model with best mse is chosen and logged into mlflow throught tensorflow.log_model method using MLFloww Tracking and MLflow Registry.</p> <p>Differents optimization tests have been done to preselect some parameters : - pas_temps has been fixe to 14 (14 days are used to predict the next day price) - A number of neuron equal to 350 has been fixed after optimization tests, with a compromise between duration of the training and performance. - Bacth size was used as the main parameter to vary during the training of the model.</p>"},{"location":"modeling/#prediction-and-evaluation","title":"Prediction and evaluation","text":"<p>This step loads data, normalizes and create a test dataset in order to predict the next price of Bitcoin  Then the best model thought is loaded through mlflow module tensorflow.load_model in order to make predictions. The script returns the predicted value as a dictionary. For each prediction, it records prediction metrics and the score (mse score on test dataset) and compares them to a reference for mse (fixed to a value of 100000, see Monitoring).</p>"},{"location":"modeling/#tesaurus","title":"tesaurus","text":"<p>[list all terms used related to MLFLOW, LTSM, RNN ...with description. See frederic for more terms to explain]</p> <ul> <li>MLflow: A platform for managing the end-to-end machine learning lifecycle.</li> <li>LSTM: Long Short-Term Memory, a type of recurrent neural network.</li> <li>RNN: Recurrent Neural Network, a class of neural networks for processing sequential data.</li> <li>OHLC: Open-High-Low-Close, a format for financial data.</li> <li>MSE: Mean Squared Error, a metric for evaluating model performance.</li> <li>MAE: Mean Absolute Error, a metric for evaluating model performance.</li> <li>MAPE: Mean Absolute Percentage Error, a metric for evaluating model performance.</li> <li>FastAPI: A web framework for building APIs with Python.</li> <li>Prometheus: An open-source monitoring and alerting toolkit.</li> <li>Kibana: An open-source analytics and visualization platform.</li> <li>Streamlit: An open-source app framework for Machine Learning and Data Science teams.</li> </ul>"},{"location":"modeling/#references","title":"References","text":"<p>[list related LTSM currency prediction usages, science paper, tests. And read/resume them !!]</p>"},{"location":"modeling/#scientific-references","title":"Scientific references","text":"<ol> <li>Liu S, Liao G, Ding Y. Stock Transaction Prediction Modeling and Analysis Based on LSTM. In Proceedings of the 2018 13th IEEE Conference on Industrial Electronics and Applications (ICIEA), Wuhan, China, 18\u201322 August 2018.</li> <li>Borovkova S, Tsiamas I. An Ensemble of LSTM Neural Networks for High \u2010 Frequency Stock Market Classification. Journal of Forecasting 2019; 38(6): 600\u2013619.</li> <li>Baek Y, Kim HY. ModAugNet: A New Forecasting Framework for Stock Market Index Value With an Overfitting Prevention LSTM Module and a Prediction LSTM Module. Expert Systems with Applications 2018; 113: 457\u2013480</li> <li>Eapen J, Bein D, Verma A. Novel Deep Learning Model With CNN and Bi-Directional LSTM for Improved Stock Market Index Prediction. In Proceedings of the 2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC), Las Vegas, NV, USA, 7\u20139 January 2019.</li> <li>Selvin S, Vinayakumar R, Gopalakrishnan EA, Menon VK, Soman KP. Stock Price Prediction Using LSTM, RNN and CNN-Sliding Window Model. In Proceedings of the 2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI), Udupi, India, 13\u201316 September 2017.</li> <li>Nelson DM, Pereira AC, de Oliveira RA. Stock Market\u2019S Price Movement Prediction With LSTM Neural Networks. In Proceedings of the 2017 International Joint Conference on Neural Networks (IJCNN), Anchorage, AK, USA, 14\u201319 May 2017.</li> <li>Sherstinsky A. Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network. Physica D: Nonlinear Phenomena 2020; 404: 132306.</li> <li>Nabipour M, Nayyeri P, Jabani H, Mosavi A, Salwana E, Shahab S. Deep Learning for Stock Market Prediction. Entropy 2020; 22(8): 840.</li> <li>Bhandari HN, Rimal B, Pokhrel NR, Rimal R, Dahal KR, Khatri RKC. Predicting Stock Market Index Using LSTM. Machine Learning with Applications 2020; 9: 100320.</li> <li>Althelaya KA, El-Alfy E-SM, Mohammed S. Evaluation of Bidirectional LSTM for Short-and Long-Term Stock Market Prediction. In Proceedings of the 2018 9th International Conference on Information and Communication Systems (ICICS), Irbid, Jordan, 3\u20135 April 2018</li> <li>Moghar A, Hamiche M. Stock Market Prediction Using LSTM Recurrent Neural Network. Procedia Computer Science 2020; 170: 1168\u20131173.</li> <li>Burak G\u00fclmez. Stock price prediction with optimized deep LSTM network with artificial rabbits optimization algorithm</li> </ol>"},{"location":"modeling/#similar-project-and-scientific-paper","title":"Similar project and scientific paper","text":"<p>Lien n\u00b01 Lien n\u00b02 Lien n\u00b03 Lien n\u00b04 Lien n\u00b05 Lien n\u00b06 Lien n\u00b07 Lien n\u00b08 [Lien n\u00b09] https://www.sciencedirect.com/science/article/pii/S0957417423008485</p> <p></p>"},{"location":"monitoring/","title":"Monitoring and Alerting Services","text":""},{"location":"monitoring/#monitoring-services","title":"Monitoring Services","text":"<p>Monitoring system is composed of several services :</p>"},{"location":"monitoring/#prometheus","title":"Prometheus :","text":"<p>Prometheus is an open-source monitoring and alerting tool, commonly used for time-series data reporting. It is particularly convenient when using Grafana as a reporting UI since Prometheus is a supported datasource. Service is available on browser port 9090.</p> <p>Prometheus is connected to services (Prediction APIs, Airflow, node-explorer, AlertManager) to retrieve metrics. It is configured using the prometheus.yml file, which is set up to scrape logs from those different services</p> <p>Classical scheme is the following :</p> <p></p> <p>We have use another classical approah using Stats_exporter. The StatsD exporter is a drop-in replacement for StatsD, used to collect various metrics and convert them to Prometheus format automaticly via configured mapping rules. tatsD's repeater backend is configured to repeat all received metrics to a statsd_exporter process.</p> <p>For example, Airflow emits metrics in the StatsD format automatically if certain environment variables (starting with AIRFLOW__SCHEDULER__STATSD_) are set.</p> <p>New schemes are the following :</p> <p></p>"},{"location":"monitoring/#node-exporter","title":"Node-exporter:","text":"<p>Node Exporter is a tool designed to collect and expose various system-level metrics from a target (node or machine).  It runs as a service on the node and provides valuable insights into CPU usage, memory consumption, disk usage, network statistics, and other crucial system-level data.  Node Exporter allows Prometheus to scrape these metrics using the pull model and store them as time-series data. Native service is available on browser port 9100.</p>"},{"location":"monitoring/#grafana","title":"Grafana","text":"<p>Grafana is an open-source reporting UI layer that is often used to connect to non-relational database.It used for the graphical interface of logs and alerts, when  connected to Prometheus to retrieve metrics. Grafana allow to create dashboards and alerts that can act on data from any of our supported data sources. It also contain its own Alertmanager system. Service is available on browser port 30020.</p>"},{"location":"monitoring/#dashboard","title":"Dashboard","text":"<p>It allows different metrics to be displayed as dashboards, making it easier to understand and analyze data. We have used the Node Exporter Full dashboard (dashboard\u2019s ID: 1860) to make Dashboard of Prometheus job (Node explorer and Airflow ann API) There are many  useful StatsD metrics made available by Airflow, such as Queued task, Runnings tasks, DAG and task duration. </p>"},{"location":"monitoring/#alertmanager","title":"Alertmanager","text":"<p>Grafana also contain its own Alertmanager system.</p>"},{"location":"monitoring/#alerting-services","title":"Alerting Services","text":""},{"location":"monitoring/#alertmanager_1","title":"Alertmanager","text":"<p>Alertmanager is responsible for managing and handling alerts generated by Prometheus based on predefined alerting rules. Alertmanager handles deduplication, grouping, and routing of alerts to different alert notification channels such as email. The monitoring container will send the alert and, if necessary, call Airflow for automatic retraining.</p> <p>Alertmanager is realized directly trought configuration files :     -monitoting_rules.yml file will be responsible for configuring alerts using PromQL.     -alertmanager.yml configuration file facilitates the setup and configuration of alerts.</p> <p>Native Service is available on browser port 9093.</p>"},{"location":"monitoring/#grafana-alertmanager","title":"Grafana Alertmanager","text":"<p>Grafana have its own AlerManager with an UI interface, selecting metrics and conditions. It is also possible to use a configuration file, which configure alert rules and contacts points. This tool can used different alert notification channels such as email, Slack , etc.</p> <p>Metrics is sent for each prediction step by Airflow to Statsd Exporter, then send to Alertmanager and Grafana own alertmanager</p>"},{"location":"monitoring/#practical-implementation","title":"Practical implementation","text":""},{"location":"monitoring/#decision","title":"Decision","text":"<p>For our project, we decided to establish a manual retraining when receiving alerts indicating that mse is greater than 1000, throught metrics model_score. This would ensure that the prediction model remains performant and up-to-date. </p> <p>We used both approach but Grafana AlerManager for alerting in model score. Alerting system have be done through Grafana Alertmanager, through configurations files, toward a Slack canal</p>"},{"location":"monitoring/#structure","title":"Structure","text":"<pre><code>Main Repo/\n\u251c\u2500\u2500 prom/\n\u2502   \u251c\u2500\u2500 __init__.py\u2502   \n\u2502   \u251c\u2500\u2500 prometheus.yml  # main configuration file for Prometheus\n\u2502   \u251c\u2500\u2500 Alermanager/\n\u2502   \u2502   \u251c\u2500\u2500 alertmanager.yml  # main configuration file for ALertmanager\n\u2502   \u2502   \u251c\u2500\u2500 configuring_rules.py  # Rules for metrics to be considered\n\u2502   \u251c\u2500\u2500 node-exporter/\n\u2502   \u2502   \u251c\u2500\u2500 proc/\n\u2502   \u2502   \u251c\u2500\u2500 sys/\n\u2502   \u251c\u2500\u2500 grafana/\n\u2502   \u2502   \u251c\u2500\u2500 provisionning/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 alerting/*\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 alert_rules.yml  # alert rules for Grafana\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 datasources/*\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 prometheus.yaml  # data source configuration for Grafana (ie Prometheus)\n\u2502   \u251c\u2500\u2500 statsd/\n\u2502   \u2502   \u251c\u2500\u2500 first_model_score.py.yml  # Script to send a first score to stasd and initiate alerting\n\u2502   \u2502   \u251c\u2500\u2500 stats_mapping.yml  # configration file to map metrics between statsd and other services like Airflow\n</code></pre>"},{"location":"project/","title":"Project","text":"<p>All project has been managed through github</p> <ul> <li>project plan + roadmap</li> <li>issues</li> </ul> <p>We used a flavor of XP programming to enhanced collaboration and communication (3 meetings per week included 1 to 1 dedicated meeting if necessary). </p> <p>We were focused on delivery through short delivery cycle  (more than 5 times per week) to get rapid feedback, and to keep motivation up ! </p> Note <p>See github statistics</p> <p>The following part sumup the setup</p>"},{"location":"project/#project-setup-and-environment-configuration","title":"Project Setup and Environment Configuration","text":""},{"location":"project/#tasks","title":"Tasks","text":"<ul> <li>Set up the development environment (Docker, virtual environments, etc.).</li> <li>Install necessary libraries and frameworks (Airflow, MLflow, etc.).</li> <li>Set up GitHub repository for version control.</li> </ul>"},{"location":"project/#deliverables","title":"Deliverables","text":"<ul> <li>Dockerfile and docker-compose.yml for each services</li> <li>Environment configuration files</li> <li>Initial commit to GitHub</li> </ul>"},{"location":"project/#data-collection-and-preprocessing","title":"Data Collection and Preprocessing","text":""},{"location":"project/#tasks_1","title":"Tasks","text":"<ul> <li>Create Airflow DAG for data extraction from Kraken.</li> <li>Preprocess the data (cleaning, normalization, etc.).</li> <li>Store processed data in PostgreSQL database.</li> </ul>"},{"location":"project/#deliverables_1","title":"Deliverables","text":"<ul> <li>Airflow DAG script for data collection</li> <li>Data preprocessing Python scripts</li> <li>PostgreSQL schema and data</li> </ul>"},{"location":"project/#model-development","title":"Model Development","text":""},{"location":"project/#tasks_2","title":"Tasks","text":"<ul> <li>Develop the LSTM model for Bitcoin price prediction.</li> <li>Split data into training, validation, and test sets.</li> <li>Train the LSTM model and evaluate its performance.</li> </ul>"},{"location":"project/#deliverables_2","title":"Deliverables","text":"<ul> <li>LSTM model script</li> <li>Trained model artifacts</li> <li>Model evaluation metrics</li> </ul>"},{"location":"project/#model-tracking-and-versioning","title":"Model Tracking and Versioning","text":""},{"location":"project/#tasks_3","title":"Tasks","text":"<ul> <li>Integrate MLflow for model tracking and versioning.</li> <li>Log model parameters, metrics, and artifacts in MLflow.</li> </ul>"},{"location":"project/#deliverables_3","title":"Deliverables","text":"<ul> <li>MLflow integration script</li> <li>MLflow server setup</li> <li>Model version tracking in MLflow</li> </ul>"},{"location":"project/#api-development","title":"API Development","text":""},{"location":"project/#tasks_4","title":"Tasks","text":"<ul> <li>Develop a private API for sensitive operations (training, etc.)</li> <li>Develop a bridge API for communication with the frontend, the private API and other services</li> <li>Create endpoints for data input and output.</li> </ul>"},{"location":"project/#deliverables_4","title":"Deliverables","text":"<ul> <li>FastAPI application for model management (pr\u00e9diction, training, etc.)</li> <li>FastAPI application for authentication and communication with the frontend</li> <li>API documentation</li> </ul>"},{"location":"project/#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"project/#tasks_5","title":"Tasks","text":"<ul> <li>Set up Prometheus for monitoring logs and metrics.</li> <li>Configure Prometheus to trigger Airflow DAGs based on log metrics.</li> <li>Integrate Grafana for log visualization and alerting.</li> </ul>"},{"location":"project/#deliverables_5","title":"Deliverables","text":"<ul> <li>Prometheus configuration files</li> <li>Kibana dashboards</li> <li>Alerts and triggers configuration</li> </ul>"},{"location":"project/#visualization-dashboard","title":"Visualization Dashboard","text":""},{"location":"project/#tasks_6","title":"Tasks","text":"<ul> <li>Develop Streamlit dashboard for visualizing model predictions and metrics, manage users and assets.</li> <li>Integrate Streamlit with Gateway API to access backend services.</li> </ul>"},{"location":"project/#deliverables_6","title":"Deliverables","text":"<ul> <li>Streamlit application.</li> </ul>"},{"location":"project/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"project/#tasks_7","title":"Tasks","text":"<ul> <li>Perform end-to-end testing of the MLOps pipeline.</li> <li>Validate the accuracy and reliability of predictions.</li> <li>Gather feedback from stakeholders and make necessary adjustments.</li> </ul>"},{"location":"project/#deliverables_7","title":"Deliverables","text":"<ul> <li>Test cases and results</li> <li>Validation report</li> </ul>"},{"location":"project/#deployment","title":"Deployment","text":""},{"location":"project/#tasks_8","title":"Tasks","text":"<ul> <li>Deploy the entire MLOps pipeline to a developement environment included unit tests</li> <li>Deploy the entire MLOps pipeline to a production environment (VM EC2 DataScientest).</li> <li>Ensure all components are correctly integrated and functional.</li> </ul>"},{"location":"project/#deliverables_8","title":"Deliverables","text":"<ul> <li>Deployment scripts under Githhub Actions</li> <li>Live development system with test</li> <li>Live production system</li> </ul>"},{"location":"project/#documentation","title":"Documentation","text":""},{"location":"project/#tasks_9","title":"Tasks:","text":"<ul> <li>Document the entire project, including setup instructions, usage guidelines, and technical details.</li> <li>Prepare a presentation for the jury explaining the project objectives, process, and outcomes.</li> </ul>"},{"location":"project/#deliverables_9","title":"Deliverables","text":"<ul> <li>Comprehensive project documentation</li> <li>Presentation document under MkDocs</li> </ul>"},{"location":"project/#project-timeline","title":"Project Timeline","text":"<ul> <li>Week 1-2: Project Setup and Environment Configuration</li> <li>Week 3-4: Data Collection and Preprocessing</li> <li>Week 5-6: Model Development</li> <li>Week 7-8: Model Tracking and Versioning</li> <li>Week 9: API Development</li> <li>Week 10-11: Monitoring and Logging</li> <li>Week 12: Visualization Dashboard</li> <li>Week 13-14: Testing and Validation</li> <li>Week 15: Deployment </li> <li>Week 16: Documentation </li> </ul> Note <p>Github project</p>"},{"location":"prometheus/","title":"Prometheus","text":"<p>This part presents an overview of Prometheus tool for educational purposes.</p>"},{"location":"prometheus/#introduction-to-prometheus","title":"Introduction to Prometheus","text":""},{"location":"prometheus/#overview","title":"Overview","text":"<p>Prometheus is an open-source monitoring and alerting toolkit designed to be highly reliable and scalable. Originally developed at SoundCloud, Prometheus has become a leading tool for monitoring systems and applications.</p> <p>Its ability to collect and store metrics as time series data, query it using PromQL, and alert on specified conditions has made it an indispensable tool in the industry.</p> <p>In 2016, Prometheus joined the Cloud Native Computing Foundation (CNCF), further solidifying its place in the ecosystem of cloud-native tools.</p> CreatorUse Cases <ul> <li>Name: Created in 2012 by engineers at SoundCloud to address their monitoring needs</li> <li>Background: Sebasti\u00e1n is a software developer from Colombia who currently resides in Berlin, Germany</li> <li>Contribution: He is the creator of several popular open-source projects, including FastAPI, Typer, SQLModel, and Asyncer</li> </ul> <ul> <li> <p>Infrastructure Monitoring</p> <ul> <li>Servers: Monitor CPU, memory, disk usage, and network traffic of physical and virtual servers.</li> <li>Containers: Track resource usage and performance metrics of Docker containers and Kubernetes pods.</li> <li>Databases: Monitor performance metrics of databases like PostgreSQL, MySQL, and MongoDB.</li> <li>Network Devices: Collect metrics from switches, routers, and firewalls to monitor network performance.</li> </ul> </li> <li> <p>Application Performance Monitoring (APM)</p> <ul> <li>Web Applications: Measure response times, error rates, and request rates for web applications.</li> <li>Microservices: Track metrics for individual microservices to monitor their performance and health.</li> <li>APIs: Monitor API endpoints for latency, throughput, and error rates.</li> </ul> </li> <li> <p>DevOps and Continuous Integration/Continuous Deployment (CI/CD)</p> <ul> <li>Build Pipelines: Monitor the status and performance of CI/CD pipelines, including build times and failure rates.</li> <li>Deployment Monitoring: Track the deployment status and measure the impact of new releases on application performance.</li> </ul> </li> <li> <p>Custom Business Metrics</p> <ul> <li>E-commerce: Monitor transaction rates, shopping cart abandonment, and user activity on e-commerce platforms.</li> <li>Finance: Track trading volumes, transaction processing times, and system availability for financial services.</li> <li>Healthcare: Monitor patient data processing, system uptime, and resource utilization in healthcare applications.</li> </ul> </li> <li> <p>Security Monitoring</p> <ul> <li>Intrusion Detection: Collect and analyze logs from intrusion detection systems (IDS) for security threats.</li> <li>Access Logs: Monitor access logs to detect unauthorized access attempts and security breaches.</li> <li>Compliance: Track compliance-related metrics to ensure adherence to security standards and regulations.</li> </ul> </li> <li> <p>Real-time Analytics</p> <ul> <li>User Behavior: Monitor real-time user activity and behavior on websites and applications.</li> <li>Social Media: Track metrics from social media platforms to analyze engagement and reach.</li> <li>IoT Devices: Collect and analyze data from Internet of Things (IoT) devices for real-time insights.</li> </ul> </li> <li> <p>Alerting and Incident Management</p> <ul> <li>Service Level Objectives (SLOs): Monitor SLOs and alert on breaches to maintain service reliability.</li> <li>Incident Response: Trigger alerts for critical incidents and integrate with alerting systems like PagerDuty or Slack.</li> <li>Automated Remediation: Use metrics to automate responses to incidents and reduce downtime.</li> </ul> </li> </ul>"},{"location":"prometheus/#main-characteristics-of-prometheus","title":"Main Characteristics of Prometheus","text":""},{"location":"prometheus/#multi-dimensional-data-model","title":"Multi-Dimensional Data Model","text":"<p>Prometheus utilizes a multi-dimensional data model where time series data is identified by a combination of a metric name and key-value pairs. </p> <p>This approach allows for the collection and storage of high-dimensional data, making it easier to query and analyze performance metrics across various dimensions like instance, job, and region.</p>"},{"location":"prometheus/#promql","title":"PromQL","text":"<p>PromQL (Prometheus Query Language) is a powerful and flexible query language designed specifically for Prometheus. It allows users to select and aggregate time series data using a rich set of operators and functions. </p> <p>PromQL leverages the multi-dimensional data model to enable complex queries that can slice and dice the data in numerous ways, providing deep insights into system performance and behavior.</p>"},{"location":"prometheus/#no-distributed-storage","title":"No Distributed Storage","text":"<p>Prometheus is designed to operate without distributed storage. Each Prometheus server node functions independently, storing its time series data locally. </p> <p>This approach simplifies the architecture and ensures that each node is autonomous, providing resilience and ease of management.</p>"},{"location":"prometheus/#pull-model","title":"Pull Model","text":"<p>Prometheus employs a pull model for time series data collection. It scrapes metrics over HTTP from instrumented services at regular intervals. </p> <p>This pull-based approach allows Prometheus to control the data collection process, ensuring consistency and reliability in the metrics gathered.</p>"},{"location":"prometheus/#push-gateway","title":"Push Gateway","text":"<p>The Push Gateway supports scenarios where the pull model is not feasible, such as short-lived jobs. It acts as an intermediary gateway, allowing clients to push metrics to it, which are then scraped by Prometheus. </p> <p>This ensures that even ephemeral tasks can be monitored effectively.</p>"},{"location":"prometheus/#service-discovery","title":"Service Discovery","text":"<p>Service discovery in Prometheus automates the process of finding and scraping targets. It supports various service discovery mechanisms like Kubernetes, Consul, and static configuration. </p> <p>This flexibility ensures that Prometheus can dynamically adapt to changes in the infrastructure, such as scaling up or down.</p>"},{"location":"prometheus/#graphing-and-dashboarding","title":"Graphing and Dashboarding","text":"<p>Prometheus offers robust support for graphing and dashboarding, enabling users to visualize metrics in multiple ways. </p> <p>It integrates seamlessly with Grafana, a popular open-source dashboard tool, allowing for the creation of interactive and customizable dashboards that provide real-time insights into system performance.</p>"},{"location":"prometheus/#prometheus-architecture","title":"Prometheus Architecture","text":""},{"location":"prometheus/#prometheus-server","title":"Prometheus Server","text":"<p>The Prometheus server is the core component responsible for scraping and storing time series data. It retrieves metrics from configured targets, stores them in the local time series database (TSDB), and makes the data available for querying and analysis using PromQL.</p>"},{"location":"prometheus/#time-series-database-tsdb","title":"Time-Series Database (TSDB)","text":"<p>The TSDB in Prometheus is optimized for handling time series data. It stores all scraped samples locally, providing efficient data storage and retrieval. </p> <p>The TSDB ensures high performance and scalability, allowing Prometheus to handle large volumes of metrics data.</p>"},{"location":"prometheus/#targets","title":"Targets","text":"<p>Targets are the endpoints from which Prometheus scrapes metrics. </p> <p>These can be application servers, databases, or any other instrumented services that expose metrics in a format understood by Prometheus.</p>"},{"location":"prometheus/#exporters","title":"Exporters","text":"<p>Exporters are special-purpose components that expose metrics for various services. Examples include HAProxy, StatsD, Graphite, and many others. </p> <p>Exporters convert metrics from these services into a format that Prometheus can scrape and store.</p>"},{"location":"prometheus/#alertmanager","title":"Alertmanager","text":"<p>The Alertmanager handles alerts generated by Prometheus. It manages the routing and notification of alerts to various channels like email, Slack, PagerDuty, and more. </p> <p>Alertmanager also supports silencing, inhibition, and grouping of alerts to reduce noise and improve incident management</p>"},{"location":"prometheus/#client-libraries","title":"Client Libraries","text":"<p>Client libraries are available for various programming languages (e.g., Go, Python, Java, Ruby) to instrument application code.  These libraries facilitate the collection and exposure of custom application metrics that Prometheus can scrape.</p>"},{"location":"prometheus/#push-gateway_1","title":"Push Gateway","text":"<p>The Push Gateway allows for pushing metrics from short-lived jobs. It serves as an intermediary that receives and temporarily stores metrics, which Prometheus then scrapes. </p> <p>This ensures that metrics from transient tasks are not lost.</p>"},{"location":"prometheus/#promql_1","title":"PromQL:","text":"<p>PromQL is essential for querying and analyzing metrics in Prometheus. It allows users to perform complex queries, aggregations, and transformations on the collected time series data, providing valuable insights into system performance.</p>"},{"location":"prometheus/#use-cases-and-benefits","title":"Use Cases and Benefits","text":""},{"location":"prometheus/#monitoring-use-cases","title":"Monitoring Use Cases","text":"<ul> <li>Web Servers: Monitor HTTP request rates, response times, and error rates.</li> <li>Databases: Track query performance, connection counts, and resource usage.</li> <li>Kubernetes Clusters: Observe pod health, resource utilization, and cluster events.</li> </ul> <p>For more use cases see Overview </p>"},{"location":"prometheus/#benefits","title":"Benefits","text":"<ul> <li>Reliability: Prometheus is designed for high availability and resilience.</li> <li>Scalability: Handles large volumes of metrics data and scales with the infrastructure.</li> <li>Ease of Integration: Seamlessly integrates with a wide range of third-party tools and services.</li> </ul>"},{"location":"results/","title":"Modelling results and Discussion","text":""},{"location":"results/#modelling-results","title":"Modelling results","text":"<p>The following curves allow to compare crypto price predicted by the model and real prices, for both training dataset et testing dataset.</p> <p>It have been done using the following parameters: - neurons = 300 ie Architecture of LSTM model contains 300 neurons, which is an optimze number. - pas_temps = 7 which means that 7 days have been used to predict the next day price. This duration offer good prediction but valeus greater that 7 can give better results. - batch_size = 5, which refers to the number of training samples that are processed together in one forward and backward pass through the model before the model's weights are updated.  It plays a crucial role in how the model learns and generalizes from the data.</p>"},{"location":"results/#training-dataset","title":"Training dataset","text":"<p>First, the results in the whole training dataset :</p> <p></p> <p>Then results in the training dataset limited to the last 5 months :</p> <p></p> <p>We can observe that curves are very close. We now have to cheack on testing dataset if thete is not Overfitting.</p> <p>Overfitting may occurs when a model learns not only the underlying patterns in the training data but also the noise and random fluctuations. As a result, the model performs well on the training data but poorly on new, unseen data.</p>"},{"location":"results/#testing-dataset","title":"Testing dataset","text":"<p>Second, the results in the whole training dataset :</p> <p></p> <p>Then results in the testing dataset limited to the last 5 months :</p> <p></p> <p>We can observe that curves are not as close as for the training, and that the model have a good generalization capability without showwin Overfitting</p>"},{"location":"results/#discussion","title":"Discussion*","text":"<p>Modelling results are satisfactory on a short term approach even using only past bitcoin prices, for the Bitcoin cryptocurrency</p> <p>Model has been volontary keep simple due to the fact that it was not the main purpose of the projet.</p> <p>As this project may continue, we integrated in Airflow the possibility to add another assets and update them regularly according to the data frequency</p> <p>The automation in GitHub Action and Airflow, as well as for the alerting, has significantly reduced manual interventions and represents a substantial time saving. This highlights the effectiveness of a process or system in automating tasks and improving efficiency.</p>"},{"location":"streamlit/","title":"Streamlit","text":"<p>This part presents an overview of Streamlit tool for educational purposes.</p>"},{"location":"streamlit/#introduction-to-streamlit","title":"Introduction to Streamlit","text":"Link <ul> <li>https://streamlit.io/</li> <li>https://share.streamlit.io</li> </ul> <p>Streamlit is\u2014a framework for building interactive and shareable web applications for data science and machine learning projects in Python.created by Adrien Treuille, Thiago Teixeira, and Amanda Kelly.</p>"},{"location":"streamlit/#main-characteristics-of-streamlit","title":"Main Characteristics of Streamlit","text":"<p>Streamlit simplify the creation of applications with just a few lines of Python code but allowing the creation of highly interactive applications with widgets like sliders, buttons, and file uploaders.</p> <p>Streamlit automatically updates the app in real-time as code is edited and integrate well with popular visualization libraries such as Matplotlib, Plotly, and Altair.</p> <p>Deploy Streamlit is easy the ease of deploying Streamlit apps using services like Streamlit Sharing or other cloud platforms.</p>"},{"location":"streamlit/#key-points","title":"Key Points:","text":"<ul> <li>Simple and easy to use</li> <li>Highly interactive with built-in widgets</li> <li>Real-time updates during development</li> <li>Integrates with popular visualization libraries</li> <li>Easy to deploy</li> </ul>"},{"location":"streamlit/#architecture-of-streamlit","title":"Architecture of Streamlit","text":""},{"location":"streamlit/#components-overview","title":"Components Overview","text":"<ul> <li>Scripts: Explain that Streamlit apps are built using simple Python scripts.</li> <li>Widgets: Describe how widgets create interactivity within the app.</li> <li>Data and State Management: Discuss how Streamlit manages data and application state.</li> <li>Streamlit Server: Describe the role of the Streamlit server in running the application.</li> </ul>"},{"location":"streamlit/#key-points_1","title":"Key Points:","text":"<ul> <li>Python scripts form the backbone of Streamlit apps</li> <li>Widgets enable interactivity</li> <li>Streamlit handles data and state management seamlessly</li> <li>The server runs and serves the application</li> </ul>"},{"location":"streamlit/#detailed-architectural-breakdown","title":"Detailed Architectural Breakdown","text":""},{"location":"streamlit/#scripts","title":"Scripts","text":"<p>A Streamlit script is essentially a Python script that leverages the Streamlit library to create interactive web applications for data science and machine learning projects.  Here's a breakdown of the main components and structure of a typical Streamlit script</p>"},{"location":"streamlit/#importing-libraries","title":"Importing Libraries","text":"<pre><code>import streamlit as st\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre> <p>Title and Header <pre><code>st.title(\"Bitcoin Price Prediction Dashboard\")\nst.header(\"Using LSTM Model\")\n</code></pre></p> <p>Uploading and Displaying Data</p> <pre><code>uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\nif uploaded_file is not None:\n    data = pd.read_csv(uploaded_file)\n    st.dataframe(data)\n</code></pre> <p>Data Visualization</p> <pre><code>fig, ax = plt.subplots()\nax.plot(data['Date'], data['Close'])\nst.pyplot(fig)\n</code></pre>"},{"location":"streamlit/#widgets","title":"Widgets","text":"<p>Streamlit widgets are interactive elements that allow users to input data and interact with your Streamlit application in real-time. These widgets enable the creation of dynamic and responsive web applications. </p> <p>The following snippets illustrate the way to implement widgets.</p> <pre><code>import streamlit as st\n\nst.title(\"Interactive Dashboard with Streamlit Widgets\")\n\n# Button\nif st.button('Click Me'):\n    st.write('Button clicked!')\n\n# Checkbox\nagree = st.checkbox('I agree')\nif agree:\n    st.write('Thanks for agreeing!')\n\n# Radio Button\ngenre = st.radio('Choose a genre', ['Comedy', 'Drama', 'Documentary'])\nst.write(f'You selected {genre}')\n\n# Select Box\noption = st.selectbox('Pick an option', ['Option 1', 'Option 2', 'Option 3'])\nst.write(f'You selected {option}')\n\n# Multi-Select\noptions = st.multiselect('Select options', ['Option 1', 'Option 2', 'Option 3'])\nst.write('You selected:', options)\n\n# Slider\nage = st.slider('Select your age', 0, 100, 25)\nst.write(f'Your age is {age}')\n\n# Text Input\nname = st.text_input('Enter your name')\nst.write(f'Hello, {name}!')\n\n# Text Area\nfeedback = st.text_area('Your feedback')\nst.write('Thank you for your feedback!')\n\n# Number Input\nnumber = st.number_input('Enter a number', min_value=0, max_value=100, value=50)\nst.write(f'You entered {number}')\n\n# File Uploader\nuploaded_file = st.file_uploader('Choose a file')\nif uploaded_file is not None:\n    st.write('File uploaded successfully')\n\n# Date Input\ndate = st.date_input('Select a date')\nst.write(f'You selected {date}')\n\n# Color Picker\ncolor = st.color_picker('Pick a color', '#00f900')\nst.write(f'Selected color: {color}')\n</code></pre>"},{"location":"streamlit/#data-and-state-management","title":"Data and State Management","text":"<p>Discuss how Streamlit manages the state of the application using st.session_state for storing stateful information across reruns.</p> <p>Explain the concept of lazy evaluation where Streamlit only re-executes parts of the script that are affected by changes.</p> <p>Streamlit handles data and state management in a way that is both intuitive and effective for building interactive applications. Here\u2019s a detailed explanation of how Streamlit manages data and state:</p>"},{"location":"streamlit/#stateless-execution-model","title":"Stateless Execution Model","text":"<p>Streamlit operates on a stateless execution model, meaning that every time you interact with the app (e.g., by clicking a button, moving a slider), Streamlit re-runs the entire script from top to bottom. This ensures that the app remains in sync with the user's inputs.</p> <p>However, this can create challenges when you need to maintain certain states across interactions, such as user selections or input data.</p> <p>explain how streamlit management Data and State Management Streamlit handles data and state management in a way that is both intuitive and effective for building interactive applications. Here\u2019s a detailed explanation of how Streamlit manages data and state:</p>"},{"location":"streamlit/#stateless-execution-model_1","title":"Stateless Execution Model","text":"<p>Streamlit operates on a stateless execution model, meaning that every time you interact with the app (e.g., by clicking a button, moving a slider), Streamlit re-runs the entire script from top to bottom. This ensures that the app remains in sync with the user's inputs.</p> <p>However, this can create challenges when you need to maintain certain states across interactions, such as user selections or input data.</p>"},{"location":"streamlit/#widgets-and-session-state","title":"Widgets and Session State","text":"<p>Widgets as State Holders Streamlit widgets not only provide interactivity but also serve as implicit state holders. When you use a widget like st.slider or st.text_input, Streamlit remembers the state of the widget (the current value) across reruns.</p> <p><pre><code>age = st.slider('Select your age', 0, 100, 25)\nst.write(f'Your age is {age}')\nIn this example, the selected age value is remembered each time the script reruns.\n</code></pre> In this example, the selected age value is remembered each time the script reruns.</p>"},{"location":"streamlit/#session-state","title":"Session State","text":"<p>For more complex state management, Streamlit provides st.session_state, a dictionary-like object where you can store and retrieve stateful information across reruns. This allows for more advanced state management beyond what widgets offer.</p> <pre><code># Counter value is maintained and incremented across user interactions\nif 'counter' not in st.session_state:\n    st.session_state.counter = 0\n\nif st.button('Increment'):\n    st.session_state.counter += 1\nst.write(f'Counter: {st.session_state.counter}')\n</code></pre> <p>In this example, the counter value is maintained and incremented across user interactions</p>"},{"location":"streamlit/#data-caching","title":"Data Caching","text":"<p>To improve performance and avoid redundant computations, Streamlit offers caching through the @st.cache_data decorator. This allows expensive operations (e.g., data loading, heavy computations) to be executed once and reused in subsequent runs, significantly speeding up the app.</p> <pre><code>@st.cache_data\ndef load_data():\n    return pd.read_csv('large_dataset.csv')\n\ndata = load_data()\nst.dataframe(data)\n</code></pre> <p>Here, load_data will only be executed once, and the result will be cached, making subsequent interactions much faster.</p>"},{"location":"streamlit/#managing-long-running-tasks","title":"Managing Long-Running Tasks","text":"<p>For long-running tasks or processes, Streamlit provides various mechanisms such as:</p> <ul> <li>Progress Bars and Spinners: To provide visual feedback during long computations.</li> <li>Threads: To run background tasks without blocking the main app.</li> </ul> <pre><code>import time\n\nif st.button('Start Long Task'):\n    with st.spinner('Processing...'):\n        time.sleep(5)\n    st.success('Task completed!')\n</code></pre> <p>This example shows how you can indicate to users that a task is running in the background.</p>"},{"location":"streamlit/#handling-user-inputs-and-interactions","title":"Handling User Inputs and Interactions","text":"<p>Streamlit\u2019s design ensures that user inputs and interactions are straightforward to handle. When a user interacts with a widget, Streamlit re-runs the script, allowing you to dynamically react to changes. </p> <p>This reactive programming model makes it easy to build complex, interactive applications without managing the control flow explicitly.</p>"},{"location":"streamlit/#streamlit-server","title":"Streamlit Server","text":"<p>The Streamlit server plays a crucial role in running and serving Streamlit applications.</p>"},{"location":"streamlit/#running-a-streamlit-app","title":"Running a Streamlit App","text":"<p>When you run a Streamlit script using the command streamlit run your_script.py, several things happen:</p> <ul> <li> <p>Server Initialization: The Streamlit CLI initializes the Streamlit server.</p> </li> <li> <p>Script Execution: The server executes the script from top to bottom.</p> </li> <li> <p>Widget State Management: The server keeps track of widget states (e.g., values of sliders, text inputs) to maintain consistency across interactions.</p> </li> </ul>"},{"location":"streamlit/#architecture-overview","title":"Architecture Overview","text":"<p>The Streamlit server architecture consists of the following key components:</p> <ul> <li>Frontend: The user interface of the Streamlit app, which is a web page rendered in a browser.</li> <li>Backend: The Python script that defines the app's behavior and logic, executed by the Streamlit server.</li> <li>WebSocket Connection: A WebSocket connection between the frontend and backend that facilitates real-time communication and updates.</li> </ul>"},{"location":"streamlit/#frontend-backend-interaction","title":"Frontend-Backend Interaction","text":""},{"location":"streamlit/#initial-load","title":"Initial Load","text":"<p>When you first open the Streamlit app in a web browser, the frontend sends a request to the Streamlit server.</p> <p>The server responds by sending the initial rendering of the app, which includes the layout, widgets, and any static content.</p>"},{"location":"streamlit/#user-interaction","title":"User Interaction","text":"<p>When a user interacts with the app (e.g., moves a slider, clicks a button), the frontend sends this interaction data back to the server via the WebSocket connection.</p> <p>The server re-executes the script from top to bottom, taking into account the new widget state, and sends the updated view back to the frontend.</p> <p>This process ensures that the app remains interactive and responsive to user input in real-time.</p>"},{"location":"streamlit/#handling-state-and-data","title":"Handling State and Data","text":"<ul> <li> <p>Session State: The server uses st.session_state to manage stateful information across script executions, ensuring that user inputs and other stateful data are preserved.</p> </li> <li> <p>Caching: Streamlit employs caching mechanisms (@st.cache_data) to store the results of expensive operations, improving performance by avoiding redundant computations.</p> </li> </ul>"},{"location":"streamlit/#serving-the-app","title":"Serving the App","text":"<p>The Streamlit server uses HTTP to serve the app to the user's browser.</p> <p>The frontend is built using modern web technologies such as React, ensuring a responsive and interactive user experience.</p>"},{"location":"streamlit/#continuous-updates","title":"Continuous Updates","text":"<p>The WebSocket connection allows for continuous updates between the frontend and backend.</p> <p>Any changes in the script, such as modifications to data or layout, are immediately reflected in the browser, providing a seamless development experience.</p>"},{"location":"streamlit/#example-workflow","title":"Example Workflow","text":"<ol> <li> <p>Start the Server: You run your Streamlit script using streamlit run app.py.</p> </li> <li> <p>Initial Execution: The server executes the script, rendering the initial app state.</p> </li> <li> <p>User Interaction: A user interacts with the app, such as moving a slider.</p> </li> <li> <p>Re-execution: The server re-executes the script with the updated slider value.</p> </li> <li> <p>Update Frontend: The updated app state is sent to the frontend, which re-renders the UI accordingly.</p> </li> </ol>"},{"location":"streamlit/#benefits-of-the-streamlit-server-model","title":"Benefits of the Streamlit Server Model","text":"<ul> <li> <p>Real-time Interactivity: The WebSocket connection ensures that any user interaction is immediately processed and reflected in the app.</p> </li> <li> <p>Simplicity: Developers can focus on writing Python code without worrying about the complexities of front-end development.</p> </li> <li> <p>Performance: Caching and efficient state management ensure that the app remains performant, even with complex data operations.</p> </li> </ul>"},{"location":"streamlit/#integration-and-extensions","title":"Integration and Extensions","text":"<p>Streamlit is designed with flexibility and extensibility in mind, enabling seamless integration with other tools and libraries, as well as the creation of custom components to extend its functionality.  Here\u2019s an overview of Streamlit\u2019s integration capabilities and extensions</p>"},{"location":"streamlit/#integration-with-data-and-visualization-libraries","title":"Integration with Data and Visualization Libraries","text":"<p>Streamlit integrates effortlessly with a wide range of data manipulation and visualization libraries, making it a powerful tool for data scientists and developers. Some commonly used libraries include:</p>"},{"location":"streamlit/#pandas-for-data-manipulation-and-analysis","title":"Pandas: For data manipulation and analysis.","text":"<pre><code>import streamlit as st\nimport pandas as pd\n\ndf = pd.read_csv('data.csv')\nst.dataframe(df)\n</code></pre>"},{"location":"streamlit/#numpy-for-numerical-computations","title":"NumPy: For numerical computations.","text":"<pre><code>import numpy as np\n\narr = np.random.randn(100)\n</code></pre>"},{"location":"streamlit/#matplotlib-for-creating-static-animated-and-interactive-visualizations","title":"Matplotlib: For creating static, animated, and interactive visualizations.","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.plot([1, 2, 3, 4], [10, 20, 25, 30])\nst.pyplot(fig)\n</code></pre>"},{"location":"streamlit/#plotly-for-interactive-web-based-visualizations","title":"Plotly: For interactive, web-based visualizations.","text":"<pre><code>import plotly.express as px\n\ndf = px.data.iris()\nfig = px.scatter(df, x='sepal_width', y='sepal_length', color='species')\nst.plotly_chart(fig)\n</code></pre>"},{"location":"streamlit/#altair-for-declarative-statistical-visualizations","title":"Altair: For declarative statistical visualizations.","text":"<pre><code>import altair as alt\n\ndf = pd.DataFrame({\n    'a': ['A', 'B', 'C', 'D', 'E'],\n    'b': [5, 3, 6, 7, 2]\n})\nchart = alt.Chart(df).mark_bar().encode(\n    x='a',\n    y='b'\n)\nst.altair_chart(chart)\n</code></pre>"},{"location":"streamlit/#integration-with-machine-learning-libraries","title":"Integration with Machine Learning Libraries","text":"<p>Streamlit supports integration with various machine learning libraries, enabling users to build and deploy machine learning models directly within their Streamlit apps.</p>"},{"location":"streamlit/#scikit-learn-for-implementing-standard-machine-learning-algorithms","title":"scikit-learn: For implementing standard machine learning algorithms.","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\nst.write(model.predict(X_test))\n</code></pre>"},{"location":"streamlit/#tensorflow-for-developing-deep-learning-models","title":"TensorFlow: For developing deep learning models.","text":"<pre><code>import tensorflow as tf\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nst.write(model.summary())\n</code></pre>"},{"location":"streamlit/#pytorch-for-creating-flexible-and-dynamic-deep-learning-models","title":"PyTorch: For creating flexible and dynamic deep learning models.","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.fc1 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.fc1(x)\n\nmodel = SimpleModel()\nst.write(model)\n</code></pre>"},{"location":"streamlit/#custom-components","title":"Custom Components","text":"<p>Streamlit\u2019s custom components allow developers to create and integrate new functionalities that are not natively supported by Streamlit. This can be particularly useful for incorporating complex interactive elements or third-party widgets.</p> <ul> <li>Streamlit Components: Streamlit provides a components module that allows developers to create custom web components using HTML, CSS, and JavaScript. These components can then be embedded in Streamlit apps.</li> </ul> <pre><code>import streamlit.components.v1 as components\n\n# Create a simple HTML component\nhtml_code = \"\"\"\n&lt;div style='color: red;'&gt;Hello, Streamlit!&lt;/div&gt;\n\"\"\"\ncomponents.html(html_code)\n</code></pre> <ul> <li>Streamlit Component Library: A growing library of community-developed components that can be easily integrated into Streamlit apps.</li> </ul>"},{"location":"streamlit/#api-integration","title":"API Integration","text":"<p>Streamlit can also be used to call APIs and integrate external data sources, allowing dynamic data fetching and interaction.</p> <pre><code># Calling API\nimport requests\n\nresponse = requests.get('https://api.example.com/data')\ndata = response.json()\nst.write(data)\n</code></pre>"},{"location":"streamlit/#deployment-options","title":"Deployment Options","text":"<p>Streamlit offers several deployment options to share apps with others:</p> <ul> <li>Streamlit Sharing: A free hosting service provided by Streamlit for deploying Streamlit apps.</li> <li>Other Platforms: Streamlit apps can also be deployed on platforms like Heroku, AWS, GCP, and Azure.</li> </ul>"},{"location":"streamlit/#benefits-and-limitations","title":"Benefits and Limitations","text":""},{"location":"streamlit/#benefits","title":"Benefits","text":"<ul> <li>Quick and easy to develop interactive apps</li> <li>Minimal learning curve for data scientists familiar with Python</li> <li>Real-time updates streamline the development process</li> <li>Broad integration with data and visualization libraries</li> </ul>"},{"location":"streamlit/#limitations","title":"Limitations","text":"<ul> <li>Primarily designed for simple to moderately complex applications</li> <li>Performance may degrade with very large datasets or highly complex visualizations</li> <li>Limited support for multi-page applications without additional libraries</li> </ul>"},{"location":"streamlit/#conclusion","title":"Conclusion","text":"<ul> <li>Streamlit simplifies the creation of interactive data apps</li> <li>Its architecture supports rapid development and integration</li> <li>Ideal for data scientists and machine learning practitioners</li> </ul>"},{"location":"thanks/","title":"Thanks","text":""},{"location":"thanks/#gratitude-for-your-commitment-and-service","title":"Gratitude for Your Commitment and Service","text":"<p>We would like to extend our deepest gratitude to our mentor and speakers at DataScientest (*). Your dedication and commitment have been instrumental in our journey through this MLOps certification. </p> <p>Your expertise and passion have not only imparted knowledge but also inspired us to strive for excellence in the field of MLOps.</p> <p>Your invaluable insights have made a significant impact on our learning experience. We are incredibly grateful for the time and energy you have devoted to our development.</p>"},{"location":"thanks/#dev-team-members","title":"Dev team members","text":""},{"location":"thanks/#frederic","title":"[Frederic]","text":"<ul> <li>Topics: MlFlow, Monitoring</li> </ul>"},{"location":"thanks/#tristan","title":"[Tristan]","text":"<ul> <li>Topics: API, Frontend</li> </ul>"},{"location":"thanks/#yann","title":"[Yann]","text":"<ul> <li>Topics: Devops, Airflow, Database, Documentation</li> </ul>"},{"location":"thanks/#conclusion","title":"Conclusion","text":"<p>Once again, thank you to all mentors and speakers of DataScientest for your invaluable contributions. Your dedication has not only enriched our knowledge but also motivated us to pursue excellence in our careers. We look forward to continuing this journey with the skills and insights we have gained from you.</p> <p>(*) We are still awaiting signed mug from our mentor ;-) </p>"}]}